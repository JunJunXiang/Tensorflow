{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\tommy\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Dimension\n",
    "num_input = 28# MNIST data input (image  shape: 28x28) \n",
    "timesteps = 28 # Timesteps\n",
    "n_classes = 10# Number of classes, one class per digit \n",
    "num_hidden_units = 128 # Number of hidden units of the RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(mode='train'):\n",
    "    \"\"\"\n",
    "    Function to (download and) load the MNIST data\n",
    "    :param mode: train or test\n",
    "    :return: images and the corresponding labels\n",
    "    \"\"\"\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets (\" MNIST_data/\", one_hot=True)\n",
    "    if mode == 'train':\n",
    "        x_train, y_train, x_valid, y_valid = mnist.train.images, mnist.train.labels, \\\n",
    "                                            mnist.validation.images, mnist.validation.labels\n",
    "        return x_train, y_train, x_valid, y_valid\n",
    "    elif mode == 'test':\n",
    "        x_test, y_test = mnist.test.images, mnist.test.labels\n",
    "    return x_test, y_test\n",
    "                                   \n",
    "def randomize(x, y):\n",
    "    \"\"\"\" Randomizes the order of data samples and their corresponding labels\"\"\"\n",
    "    permutation = np.random.permutation(y. shape [0])\n",
    "    shuffled_x = x[permutation, :]\n",
    "    shuffled_y = y[permutation]\n",
    "    return shuffled_x, shuffled_y\n",
    "                                   \n",
    "def get_next_batch(x, y, start, end):\n",
    "    x_batch = x[start:end]\n",
    "    y_batch = y[start:end]\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-3ce5f0599434>:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\Users\\tommy\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\Users\\tommy\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting  MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Users\\tommy\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting  MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Users\\tommy\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting  MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting  MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Users\\tommy\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Size of:\n",
      "- Training-set:\t\t55000\n",
      "- Validation-set:\t5000\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid = load_data(mode='train')\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(y_train)))\n",
    "print(\"- Validation-set:\\t{}\".format(len(y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001# The optimization initial Learning rate\n",
    "epochs = 10 #Total number of training epochs \n",
    "batch_size = 100# Training batch size\n",
    "display_freq=100 # Frequency of displaying the training  results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight and bais wrappers\n",
    "def weight_variable(shape):\n",
    "    \"\"\"\n",
    "    Create a weight variable with appropriate initialization\n",
    "    :param name: weight name\n",
    "    :param shape: weight shape\n",
    "    :return: initialized weight variable\n",
    "    \"\"\"\n",
    "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    return tf.get_variable('w',\n",
    "                            dtype=tf.float32,\n",
    "                            shape=shape,\n",
    "                            initializer=initer)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"\"\n",
    "    Create a bias variable with appropriate initialization\n",
    "    :param name: bias variable name\n",
    "    :param shape: bias variable shape\n",
    "    :return: initialized bias variable\n",
    "    \"\"\"\n",
    "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable('b',\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases, timesteps, num_hidden):\n",
    "    # Prepare data shape to match \"rnn\" function requirements\n",
    "    # Current data  input shape:  (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors List of shape (batch_size, n_input)\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "    # Define a rnn cell with tensorflow\n",
    "    rnn_cell = rnn.BasicRNNCell(num_hidden)\n",
    "    # Get lstm cell output\n",
    "    # If no initial_state is provided, dtype must be  specified\n",
    "    # If no initial cell state is provided, they will be initialized to zerd\n",
    "    states_series, current_state = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "    # Linear activation, using rnn  inner Loop Last output\n",
    "    return tf.matmul(current_state, weights) + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLaceholders for inputs (x) and outputs(y)\n",
    "x = tf.placeholder(tf.float32, shape=[None, timesteps, num_input], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=[None, n_classes], name='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create weight matrix initialized randomely from N~(0, 0.01)\n",
    "W = weight_variable(shape=[num_hidden_units, n_classes])\n",
    "\n",
    "# create bias vector initialized as zero   \n",
    "b = bias_variable(shape=[n_classes])\n",
    "                         \n",
    "output_logits = RNN(x, W, b, timesteps, num_hidden_units)\n",
    "y_pred = tf.nn.softmax(output_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-b542844d6d25>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model predictions\n",
    "cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')\n",
    "                           \n",
    "# Define the Loss function, optimizer, and accuracy\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')\n",
    "optimizer =tf.train.AdamOptimizer(learning_rate=learning_rate, name= 'Adam-op').minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name='correct_pred')\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name= 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the op for initializing all variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch:1\n",
      "iter   0:\t Loss=2.28,\tTraining Accuracy=23.0%\n",
      "iter 100:\t Loss=0.82,\tTraining Accuracy=73.0%\n",
      "iter 200:\t Loss=0.57,\tTraining Accuracy=86.0%\n",
      "iter 300:\t Loss=0.51,\tTraining Accuracy=84.0%\n",
      "iter 400:\t Loss=0.42,\tTraining Accuracy=86.0%\n",
      "iter 500:\t Loss=0.22,\tTraining Accuracy=95.0%\n",
      "Training epoch:2\n",
      "iter   0:\t Loss=0.22,\tTraining Accuracy=92.0%\n",
      "iter 100:\t Loss=0.24,\tTraining Accuracy=94.0%\n",
      "iter 200:\t Loss=0.64,\tTraining Accuracy=85.0%\n",
      "iter 300:\t Loss=0.38,\tTraining Accuracy=90.0%\n",
      "iter 400:\t Loss=0.21,\tTraining Accuracy=92.0%\n",
      "iter 500:\t Loss=0.30,\tTraining Accuracy=89.0%\n",
      "Training epoch:3\n",
      "iter   0:\t Loss=0.26,\tTraining Accuracy=93.0%\n",
      "iter 100:\t Loss=0.22,\tTraining Accuracy=93.0%\n",
      "iter 200:\t Loss=0.32,\tTraining Accuracy=91.0%\n",
      "iter 300:\t Loss=0.16,\tTraining Accuracy=94.0%\n",
      "iter 400:\t Loss=0.26,\tTraining Accuracy=95.0%\n",
      "iter 500:\t Loss=0.10,\tTraining Accuracy=98.0%\n",
      "Training epoch:4\n",
      "iter   0:\t Loss=0.11,\tTraining Accuracy=96.0%\n",
      "iter 100:\t Loss=0.18,\tTraining Accuracy=95.0%\n",
      "iter 200:\t Loss=0.12,\tTraining Accuracy=97.0%\n",
      "iter 300:\t Loss=0.14,\tTraining Accuracy=93.0%\n",
      "iter 400:\t Loss=0.23,\tTraining Accuracy=94.0%\n",
      "iter 500:\t Loss=0.18,\tTraining Accuracy=96.0%\n",
      "Training epoch:5\n",
      "iter   0:\t Loss=0.17,\tTraining Accuracy=95.0%\n",
      "iter 100:\t Loss=0.09,\tTraining Accuracy=97.0%\n",
      "iter 200:\t Loss=0.20,\tTraining Accuracy=92.0%\n",
      "iter 300:\t Loss=0.16,\tTraining Accuracy=96.0%\n",
      "iter 400:\t Loss=0.17,\tTraining Accuracy=95.0%\n",
      "iter 500:\t Loss=0.16,\tTraining Accuracy=95.0%\n",
      "Training epoch:6\n",
      "iter   0:\t Loss=0.14,\tTraining Accuracy=95.0%\n",
      "iter 100:\t Loss=0.06,\tTraining Accuracy=99.0%\n",
      "iter 200:\t Loss=0.11,\tTraining Accuracy=97.0%\n",
      "iter 300:\t Loss=0.10,\tTraining Accuracy=97.0%\n",
      "iter 400:\t Loss=0.04,\tTraining Accuracy=100.0%\n",
      "iter 500:\t Loss=0.16,\tTraining Accuracy=97.0%\n",
      "Training epoch:7\n",
      "iter   0:\t Loss=0.09,\tTraining Accuracy=98.0%\n",
      "iter 100:\t Loss=0.06,\tTraining Accuracy=99.0%\n",
      "iter 200:\t Loss=0.06,\tTraining Accuracy=98.0%\n",
      "iter 300:\t Loss=0.09,\tTraining Accuracy=98.0%\n",
      "iter 400:\t Loss=0.11,\tTraining Accuracy=99.0%\n",
      "iter 500:\t Loss=0.13,\tTraining Accuracy=96.0%\n",
      "Training epoch:8\n",
      "iter   0:\t Loss=0.16,\tTraining Accuracy=95.0%\n",
      "iter 100:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 200:\t Loss=0.09,\tTraining Accuracy=97.0%\n",
      "iter 300:\t Loss=0.11,\tTraining Accuracy=97.0%\n",
      "iter 400:\t Loss=0.07,\tTraining Accuracy=97.0%\n",
      "iter 500:\t Loss=0.03,\tTraining Accuracy=100.0%\n",
      "Training epoch:9\n",
      "iter   0:\t Loss=0.15,\tTraining Accuracy=96.0%\n",
      "iter 100:\t Loss=0.14,\tTraining Accuracy=98.0%\n",
      "iter 200:\t Loss=0.06,\tTraining Accuracy=98.0%\n",
      "iter 300:\t Loss=0.25,\tTraining Accuracy=92.0%\n",
      "iter 400:\t Loss=0.21,\tTraining Accuracy=97.0%\n",
      "iter 500:\t Loss=0.05,\tTraining Accuracy=99.0%\n",
      "Training epoch:10\n",
      "iter   0:\t Loss=0.06,\tTraining Accuracy=99.0%\n",
      "iter 100:\t Loss=0.18,\tTraining Accuracy=97.0%\n",
      "iter 200:\t Loss=0.07,\tTraining Accuracy=97.0%\n",
      "iter 300:\t Loss=0.16,\tTraining Accuracy=96.0%\n",
      "iter 400:\t Loss=0.19,\tTraining Accuracy=94.0%\n",
      "iter 500:\t Loss=0.15,\tTraining Accuracy=97.0%\n",
      "----------------------------------\n",
      "Epoch:  (0), validation loss: 0.16, validation accuracy: 96.0%\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "global_step = 0\n",
    "# Number of training iterations in each epoch\n",
    "num_tr_iter = int(len(y_train) / batch_size)\n",
    "for epoch in range(epochs):\n",
    "    print('Training epoch:{}'.format(epoch + 1))\n",
    "    x_train, y_train = randomize(x_train, y_train)\n",
    "    for iteration in range(num_tr_iter):\n",
    "        global_step +=1\n",
    "        start = iteration * batch_size\n",
    "        end = (iteration + 1) * batch_size\n",
    "        x_batch, y_batch = get_next_batch(x_train, y_train, start, end)\n",
    "        x_batch = x_batch.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        feed_dict_batch = {x: x_batch, y: y_batch}\n",
    "        sess.run(optimizer, feed_dict=feed_dict_batch)\n",
    "        \n",
    "        if iteration % display_freq == 0:\n",
    "            # Calculate  and display the batch Loss and accuracy\n",
    "            loss_batch, acc_batch =  sess.run([loss, accuracy],\n",
    "                                                feed_dict=feed_dict_batch)\n",
    "            print(\"iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}\".\n",
    "                format(iteration, loss_batch, acc_batch))\n",
    "                   \n",
    "# Run validation after every epoch\n",
    "                   \n",
    "feed_dict_valid = {x: x_valid[:1000].reshape((-1, timesteps, num_input)), y: y_valid[:1000]}\n",
    "loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
    "print('----------------------------------')\n",
    "print(\"Epoch:  (0), validation loss: {1:.2f}, validation accuracy: {2:.01%}\".\n",
    "        format(epoch + 1, loss_valid, acc_valid))\n",
    "print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None, title=None):\n",
    "    \"\"\"\n",
    "    Create figure with 3x3 sub-plots.\n",
    "     :param images: array of images to be plotted, (9, img_h*img_w)\n",
    "     :param cls_true: corresponding true labels (9,)\n",
    "     :param cls_pred: corresponding true labels (9,)\n",
    "      \"\"\"\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "    # Plot image.\n",
    "        ax.imshow(np.squeeze(images[i]).reshape(28, 28), cmap='binary')\n",
    "    # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            ax_title = \"True: {0}\" .format(cls_true[i])\n",
    "        else:\n",
    "            ax_title = \"True: {0}, Pred: {1}\" .format(cls_true[i], cls_pred[i])\n",
    "        ax.set_title(ax_title)\n",
    "                 # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks ([])\n",
    "    if title:\n",
    "        plt.suptitle(title, size=20)\n",
    "    plt.show(block=False)\n",
    "def plot_example_errors(images, cls_true, cls_pred, title=None):\n",
    "    \"\"\"\n",
    "    Function for plotting examples of images that have been mis-classified\n",
    "    :param images: array of all images, (#imgs, img_h*img_w)\n",
    "    :param cls_true: corresponding true labels, (#imgs,)\n",
    "    :param cls_pred: corresponding predicted labels, (#imgs,)\n",
    "    \"\"\"          \n",
    "\n",
    "    # Negate the boolean array.\n",
    "    incorrect = np.logical_not(np.equal(cls_pred, cls_true))\n",
    "    # Get the images from the test-set that have been\n",
    "    # incorrectly classified.\n",
    "    incorrect_images = images[incorrect]\n",
    "    # Get the true and predicted classes for those images.\n",
    "    cls_pred =cls_pred[incorrect]\n",
    "    cls_true =cls_true[incorrect]\n",
    "    # Plot the first g images.\n",
    "    plot_images(images=incorrect_images [0:9],\n",
    "                cls_true=cls_true[0:9],\n",
    "                cls_pred=cls_pred[0:9],\n",
    "                title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting  MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting  MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting  MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting  MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "-----------------------------------\n",
      "Test loss: 0.14, test accuracy:  95.6%\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAggAAAI7CAYAAACJEmNgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XmYFNX1//HPAUFUlEUQFYQx4hpUXIhiXPipKME1xH0JGpeIW2KMJDGoaBTRGDUuEZcoRsQFXCFuXzUoiBsI7isCiorIIiqiINzfH1WTdPepYbp7ep15v56nn6FO3ao+NdzpPn3rdpWFEAQAAJCqWbkTAAAAlYcCAQAAOBQIAADAoUAAAAAOBQIAAHAoEAAAgEOBAKDRM7M+ZhbMbGi5cwGqBQUCKpaZbWFm15rZG2a22MyWmdmnZvZvMzvBzFqVO8diMrMJZpbzhUrM7Lj4zXBVj1lFSBlAI7JauRMAkpjZ+ZIuUFTEviDpdknfSOokqY+kWyQNkrRjmVKsBq9KerCOdV+WMhEA1YcCARXHzM6VdKGkjyUdGkJ4MaHN/pLOLnVuVWZ6CGFouZMAUJ04xYCKYmY1koZKWi6pf1JxIEkhhPGS+iVsf5iZPRufklhqZq+b2Z/MbPWEtrPixzpmdmX87+W156nNbGg8HN/HzI4ysxfN7JvM4Xkz28nMxprZ3Pg0yMdmdqOZbVjHMbY3s0viUyffxrm+ambDzWwtM6uJTy3sEbdPPTUwIetfZpbM7Hfxvu9LWLe3ma2If49rpMQPNrNRZvaemS2Jfy9TzexMM3OvK2Y2Mn6Ojc3sdDN7y8y+i3/n55qZxe0ONbOX4n3OM7Prkk4l1f4uzGxDM7sjbrs0zuGoHI+/vZldamZvx/tYbGZPmdk+CW1bxsf4ipktiv//ZpnZQ2a2dy7PC1Q6RhBQaY6X1ELS3SGEN1bVMITwfeqymQ2T9CdJ8yWNVnRK4meShkna18z6hhCWZ+ympaSnJbWX9ISkryTNzGhztqS+ksZJ+o+kNinPebykmyV9L+lhRaMem0o6UdIBZrZzCOGjlPYbx/voJmmqpBsUFeqbSTpL0ghFw/8XSjoubndhSi6zVvU7yUcI4Uoz6yNpgJmdGkL4R5zr+pJGSfpO0uEhhKUpmw2XtFLSi5I+UfQ72VPS3yX1knRsHU93haJTROMU/b4PlHSJpJZmtjDe74OSJir6nZ8mqbmi00mZ2kmarOj3dZuktpIOk3SnmXUOIfy1vmM3s26SJkiqiZ/zMUlrSdpf0mNm9usQws0pm4yUdKSkNyT9S9JSSRtK2lVRwfpkfc8JVI0QAg8eFfOQ9JSkIOnEHLfrHW/3kaT1U+KrKXozCpLOzdhmVhx/UtJaCfscGq9fImm7hPWbSVom6QNJnTPW7SlphaQHMuLPxfv8U8L+OkhqlbI8IfoTzfl3eFz8HNPjY0h69MvYZl1Fxc1SSdsqKlqejPdzfMJzbJIQa6ZorkiQtFPGupFxfFbq70rRm/r8+Hf8haQtU9atLuktRcXXehn7C/HjXknNUuIbS1oY/7/8KCXeJ24/NGM/ExQVOkdkxNvGv7+lkjrFsTZx2ymSmicc/7rl/vvhwaOQj7InwINH6iN+QwiZb2BZbHdzvN3JCes2i9+sP8yI1xYI29axz9oC4ao61l8Vr9+vjvUPSPpB0trx8g5x+2mpb2qrOKaGFgirelydsN2ucb7vKBp1CZJG5fjc28fbnZ8Rry0QTkjY5tZ43UUJ6y6I1+2REQ9xrhuv4v/tgpSYKxAUFUJB0pg6juWgeP2p8fI68fJzkqzUfxs8eJT6wSkGVBqLf+b69b7t459PZ64IIbxnZnMkbWxmbUMIqTP4v5P0Wj37fqmOeO/45x5m1ith/XqKhsc3U3Q6Yec4/ngIYWU9z1kIt4cQjsu2cQhhkpldIOliRadq3pd0SlJbM1tX0jmS+kv6kaJh+VSd63iaKQmxT+OfUxPWfRL/7JKw7qMQQubpICkqrC6QtF0dOdSq/f9rY8nXR+gY/9xSkkIIX5nZOEkHSJoez9mYKOnFEMK39TwXUHUoEFBpPpW0hZLfEFaldl7AZ3Ws/0xS17hdaoEwL4RQXzEyt474uvHPc+rZvnX8s23885O6GlaA+yVdpOh0wS0hhG8yG5hZW0kvKxrOf0nRufiFij7Rt5X0G0WnB5IsToj9kMW6FgnrPq/jOWr/v9rUsb5W7f9f3/hRl9Yp/z5c0h8kHaX/zQ35zszGSvp9CKGunICqw7cYUGkmxT/3ynG72jeX9etYv0FGu1rZjFTU1aZ2X21CCLaKxzNxu9rCpK5P12UVf1vgrnhxkaTzzWzzhKYnKioOLgwh7BRCODWEMCREX6m8pzTZSoquiZGktg8kFRypatf/pp7/v+NrNwghLA0hDA0hbKao4DxGUZ89RtLYBhwLUHEoEFBpblP0FcdfmNlWq2qY8dXFafHPPgntuisakZiZcXqhoV6If+6WY/t9k74KmGCFJJlZ81wTy9OVis7LXyrpCElrSron4WuG3eOf7muRir+aWSJdLfpabKY+8c9pCetS5fr/lyaE8HEI4U5J+yo6HbNrfOoFaBQoEFBRQgizFE0yaynp32aWeKVEM+sn6dGU0K3xzyFm1jGlXXNFX61rJumfBU73OkXFzFVmtllCji3N7L9vPiGEqYq+ltdT0TB1Zvt1M96MF8Q/uxY06wRm9gtFXyV8TtHkvickXa6oYLgyo/ms+GefjH1sp2juQqk0l3RZarEVf430TEWnJkatauMQwhRFcwgGmNmvktqY2dZmtl78745mtlNCs7UkrR0/57J8DgSoRMxBQMUJIQwzs9UUTTR72cwmK5rcVnup5d0VXWtgSso2k83sckmDJb0RnxNeoug6CD0UDQPX+734HPN8J35juVXSm2b2mKT3FJ0v76rok+kXiuZU1DpG0SS6YfGb8gRFEzM3lbRP3HZW3PYpSYdKut/MHlH0lbvZIYQ7skyxZx2T72rzHyr99+JUtyg6rXBUCGFF3GSIot/1IDN7KoRQO2LwL0XzLq42s/+n6NPzpoquHXC/ovP0pfCapJ0kTTWzJxTNOThc0TyIwSGEGVns4yhFE1v/aWZnKrquw5eKRpy2UdR3ekuap+jU0Atm9rakVxR9LXQdRce9vqRrQghfF+7wgDIr99coePCo66Fo9vi1ii5K85WiT2efKRo5OEHS6gnbHKGoGPha0TcU3pT0Z6VcXyCl7SxJs1bx/EMVzT/oU0+eWyv6Gt9sRd/ZXxjnfKOkPRParyvpMknvxjl+qeg795dIWjOlXXNFXzf8UNFIRZA0IYvf23Gq/2uOIW7bQtFQe5A0IGFf3RQVDl8q5SuFkrZSdGGoeYoKsamK5ibUxPsambGfkXG8Jpffc8qxHJcRD4qKqw0VjRTMi3+XrygqcjL300cJ10GI160t6dz4GL5RVIjNlPRvSScrvkaGosLjfEUFxSfx//VncR5Hiq8+8mhkDwsh12+TAUB5xZeifiaE0KfcuQCNFXMQAACAQ4EAAAAcCgQAAOAwBwEAADiMIAAAAIcCAQAAOBQIAADAoUAAAAAOBQIAAHAoEAAAgEOBAAAAHAoEAADgUCAAAACHAgEAADgUCAAAwKFAAAAADgUCAABwKBAAAIBDgQAAABwKBAAA4FAgAAAAhwIBAAA4FAgAAMChQAAAAA4FAgAAcCgQAACAQ4EAAAAcCgQAAOBQIAAAAIcCAQAAOBQIAADAoUAAAAAOBQIAAHAoEKqImY0ys6HlzgMoFPo0GqPG0q/LXiCY2Tcpj5VmtjRl+egy5PNERk7LzGxaltueaGYr4u2+MrNpZta/2DnXkct5GcexNM6tXTnyaUoqsE//0czeNLOvzexDM/tdDttWUp8+0Mwmm9mXZvaZmd1oZq3LkUtTVIH9ei8zmxD3yw9y3LaS+nVnMxsX9+lgZl3KkUeSshcIIYTWtQ9JH0k6ICV2Z2Z7M1utyPnsk5HTS5LG5LCLifF27ST9S9IYM2uT2agEx/GXjOP4m6SnQgiLivm8qLw+HTtGUltJ+0k6y8wOyWHbiujTktaWdKGkDST9WNLGkoYX+TkRq8B+vUTSLZL+kOf2ldKvV0p6RFIuf5MlUfYCoT5mdrGZ3WNmd5nZ15KOyRy+MbO9zWxWynIXM3vAzL4ws5lmdlqez72JpN6S7sh12xDCCkm3SlpT0sa1OZrZuWY2V9LN8XMcaGavxp+KJplZj5Tn38HMpsef/O6StHqex2GSjpV0ez7bo7BK3adDCMNDCNNCCCtCCG9LGifpp7nmXe4+HUK4M4TweAhhaQhhoaI3h5yPA8VRhn79QghhlKSZDcm7Avr1ZyGEGyRNbchxFEPFFwixn0saLamNpHtW1dDMmksaL+llSZ0l9ZV0jpntFa/fw8zmZ/m8AyX9J4Twca4Jx1XnCZK+ljQjDneR1FpSV0mnmlkvRZ3vREnrKuqkD5lZSzNbXdJDcax9/O+DU48z7qg7Z5HO/1NUJT+Q63GgaMrSp82smaRdJb2Za8IV1qclafd8jgNFVa7X6rxVYL+uGNVSIEwKIYwLIawMISytp+3OktYJIQwLISwLIXwg6Z+SjpCkEMIzIYQO9T1hyqfukTnmuquZfSlprqIho4NDCF/H636QNDTOa6mkkyX9I4Twcvzp7ta4XS9Fn4yCpGtDCMtDCHdL+u9ciLh92xDCC1nkNFDSvSGEb3M8FhRPyft07C+K+uG/csi14vq0mf1M0lGSLsjhOFB85erX+ai4fl1pSnHusxBy+QTfTVLX+D++VnNJE3J8zj0UVYr357jdpBBCnzrWfR5CWJay3E3S0WZ2VkqspaJquqWkOSGEkLJudo65yMzWkvQLST/LdVsUVcn7tJn9RtGL724Z/bA+ldand1F02m9ACGFGfe1RUuV4rc5XRfXrSlQtBULIWF6i6HxRrfVT/v2xpPdDCFs28DkHShpb4E/dmcfxsaQLQwiXZTaMh9kyZ7N2Ve5DqodI+lzSpBy3Q3GVtE+b2cmSzpa0ewjh03z3k6CkfdrMdpT0oKRfhhAm5JYqSqAcr9XFUI7X6opTLacYMk2XtJ+ZtTOzDSSdmbLueUnLzOxsM2sVn//Z2sx2yHbnZramojfWkQnrJpnZkAbmX+smSaeZWS+LtDazA+JP/ZMkNTOz081sNTM7VNL2eTzHQEm3Z1S3qDxF69NmNlDR7P++IYRZCeurok+b2baKZnufGkJ4pED5oriK2a+bmVkrSS2iRWtlZi1S1ldFv45zbaX/TWxcPZ7XUHbVWiCMlPS2omGcxyTdXbsihPCDpP6SfiJplqT5km6UtI4kmVmfjCGtJL+Q9IWkiQnrukh6rkHZ/y/XFyUNknSDpEWS3lP0dTSFEL5XNOHnpHjdAEWfnCT9d+LLN2bWu679m1lXRRO5cv4WBkpupIrXpy9WdLpsqv3ve+vXpayvlj79+/g4RqYcx6uFyBtFM1LF69d7Sloq6WFJP4r//WjK+qro1/EkyaWSao/1A0UjL2VnfLDMnpnVSLojhLBbmVMBCoI+jcaIfl0YFAgAAMCp1lMMAACgiCgQAACAQ4EAAACcnK6D0KFDh1BTU1OkVFDtpk6dOj+E0LHceeSCPo1VoU+jscmlT+dUINTU1GjKlCn5ZYVGz8yq7uph9GmsCn0ajU0ufZpTDAAAwKFAAAAADgUCAABwKBAAAIBDgQAAABwKBAAA4FAgAAAAhwIBAAA4FAgAAMChQAAAAA4FAgAAcCgQAACAQ4EAAACcnO7mCKDyXHHFFWnLS5cudW1ee+01Fxs7dmxW+x80aJCL9e7d28WOPfbYrPYHoDowggAAABwKBAAA4FAgAAAAhzkIQBU5/PDDXWzMmDF57cvMsmo3YsQIF3vyySddbI899khb7tq1a155AeXw3nvvudjmm2+etnzNNde4NmeccUbRcio3RhAAAIBDgQAAABwKBAAA4FAgAAAAh0mKQIUq5ITELbbYwsX69evnYh9++KGLPfzwwy72wQcfuNioUaPSls8999xcUgTKatq0aS7WrFn6Z+jOnTuXKp2KwAgCAABwKBAAAIBDgQAAABwKBAAA4DBJEagAU6ZMcbEHHnggq2179OiRtpw0qbBDhw4u1rp1axdbtmyZi+20004u9uqrr7rYggULVpknUMmmT5/uYpl/IwMGDChVOhWBEQQAAOBQIAAAAIcCAQAAOBQIAADAqfhJimPHjnWxm2++2cU23HBDF2vVqpWLHX300WnL66+/vmvTvXv3XFIEGuyzzz5zsRCCi2VOSJSkxx9/PG15gw02yDuPK664wsXefvvtrLbdf//9835eoJRef/11F7v22mtd7Je//GUp0qlYjCAAAACHAgEAADgUCAAAwKFAAAAATsVPUjznnHNcbNasWXnvb8SIEWnL66yzjmuz1VZb5b3/Yttoo41cbPDgwS624447liIdFMgBBxzgYkm3VF577bVdrH379gXL45577nGxpKsrAtXs3XffdbElS5a4WNIt15sSRhAAAIBDgQAAABwKBAAA4FAgAAAAp+InKd5yyy0ulnSr2aSJhW+99ZaLTZs2LW15woQJrs0LL7zgYl27dnWxjz76yMWy1aJFi7TlpNvxJl1dLym3pImLTFKsft26dSvq/v/617+62HvvvZfVtkm3gE6KAZXo8ssvd7GamhoXa+qvo4wgAAAAhwIBAAA4FAgAAMChQAAAAE7FT1Lca6+9sool6devX71tFi1a5GKZExml5MkqL7/8clZ5JFl99dXTljfffHPXZosttnCxhQsXutgmm2ySdx5oGsaPH+9i559/vot9//33LtapUycXGz58uIutueaaeWYHFE/SlXeTXruTXoPXWmutYqRUNRhBAAAADgUCAABwKBAAAIBT8XMQiq1du3Yutueee2a1bbZzIbJx3333uVjS/IhtttnGxY444oiC5YHGacqUKS6WNN8gSdId7fbYY48G5wSUwjPPPJNVu44dOxY5k+rDCAIAAHAoEAAAgEOBAAAAHAoEAADgNPlJiuUyb968tOVTTz3VtQkhuFjSxW3at29fuMRQ9Q4++GAXe/zxx7PaduDAgS528cUXNzgnoFxee+21rNoNHjy4yJlUH0YQAACAQ4EAAAAcCgQAAOBQIAAAAIdJimVy/fXXpy1nTlqUpLZt27pY0h3H0LR99tlnacuTJ092bZKumph05bghQ4a4WOvWrRuQHVA6zz//vIvddtttLrbddtu5WN++fYuSUzVjBAEAADgUCAAAwKFAAAAADgUCAABwmKRYApMmTXKx4cOH17vdQw895GI9evQoSE5oPAYMGJC2PH/+/Ky2O/roo11sk002KUhOQDk89dRTLrZo0SIX69evn4u1atWqKDlVM0YQAACAQ4EAAAAcCgQAAOBQIAAAAIdJiiXwyCOPuNiyZcvSlvfee2/Xpnfv3kXLCdXp4YcfdrFp06bVu12fPn1c7KKLLipESkDFePXVV7Nqd+ihhxY5k8aBEQQAAOBQIAAAAIcCAQAAOBQIAADAYZJigS1dutTFHnvsMRdbffXV05YvvPBC16ZFixaFSwxVZ8GCBS42bNgwF8uc8JqkZ8+eLsZtnFHN5s6d62ITJ050sS222MLFfv7znxclp8aGEQQAAOBQIAAAAIcCAQAAOBQIAADAYZJigf31r391saQr3f3sZz9LW95ll12KlhOq09/+9jcXe+mll+rd7uCDD3YxrpqIxmbkyJEu9vnnn7tY5mstsscIAgAAcCgQAACAQ4EAAAAc5iA0wPjx413sL3/5i4u1adPGxc4777yi5ITG48orr8xru+uvv97FuCgSGpvZs2dn1a5du3ZFzqTxYgQBAAA4FAgAAMChQAAAAA4FAgAAcJikmKWkO+udeeaZLvbDDz+4WP/+/V2sd+/ehUkMyJDUVwt9Z9CkibeZz7F8+XLXZvHixVntf9GiRS521VVXZZlduubNm7vYZZdd5mJrrrlmXvtHeYwbNy6rdvvvv3+RM2m8GEEAAAAOBQIAAHAoEAAAgEOBAAAAHCYpJlixYoWL9evXz8VmzpzpYt27d3expKsrAsWyzTbbFP05DjvsMBfbYIMN0paT7qx39913Fy2nXHTq1MnFhgwZUoZMkI2JEye6WFL/QmExggAAABwKBAAA4FAgAAAAhwIBAAA4TFJMMGPGDBebMmVKVtsm3aJ3k002aXBOaHqSrsD54IMPliET79577y3YvpKu8tisWXafXQ488EAX23HHHevdbtddd81q/6gMDzzwgIslXbV2u+22c7E99tijKDk1BYwgAAAAhwIBAAA4FAgAAMChQAAAAE6Tn6Q4e/ZsF9tnn32y2vaKK65wMW4tikK5//77Xezyyy93sWXLluW1/7feesvFGnKlwxNOOCFtuVu3bllt94tf/MLFttxyy7zzQPX79ttv05YfffTRrLY79NBDXSzpdt/IDiMIAADAoUAAAAAOBQIAAHAoEAAAgNPkJyneeOONLpY0cTFJ0hW6zKzBOQF1GTx4cFH3P3r06KLuH8hG5tU127Zt69ocdNBBLvab3/ymaDk1RYwgAAAAhwIBAAA4FAgAAMChQAAAAE6Tm6Q4ceLEtOXrrruuTJkAAJJkTlJ8/vnny5RJ08YIAgAAcCgQAACAQ4EAAACcJjcHYdKkSWnLX3/9dVbbde/e3cVat25dkJwAAKg0jCAAAACHAgEAADgUCAAAwKFAAAAATpObpJiNnj17uthTTz3lYu3bty9FOgAAlBwjCAAAwKFAAAAADgUCAABwKBAAAIDT5CYp/ulPf1rlMgAAYAQBAAAkoEAAAAAOBQIAAHAoEAAAgGMhhOwbm30haXbx0kGV6xZC6FjuJHJBn0Y96NNobLLu0zkVCAAAoGngFAMAAHAoEAAAgEOBAAAAHAoEAADgUCAAAACHAgEAADgUCAAAwKFAAAAADgUCAABwKBAAAIBDgQAAABwKBAAA4FAgAAAAhwIBAAA4FAgAAMChQAAAAA4FAgAAcCgQAACAQ4EAAAAcCgQAAOBQIAAAAIcCAQAAOBQIAADAoUAAAAAOBUIVMbNJZnZcufMACol+jcbGzEaZ2dBy59FQZS8QzOyblMdKM1uasnx0GfJpZmZXmNlCM1tgZpeamWW57cVmtjzO/Usze87Mdip2zqvI5xwzm2tmi83sFjNrWa5cmppK69cpea1uZu+b2awctqmofp2S1zNmFsqdR1NRaX3azPYyswlm9pWZfZDjtiea2Yo496/MbJqZ9S9WrvXk0tnMxpnZZ2YWzKxLOfJIUvYCIYTQuvYh6SNJB6TE7sxsb2arFTmlQZL6S+ohqaekAZJOyGH7O+NjWU/Si5LuS2pU7OMws/0knS3p/0naWNLmks4v5nPifyqwX9f6o6TP8tiuIvp1yvMMlJRV4Y7CqMA+vUTSLZL+kOf2E+NjaSfpX5LGmFmbzEYlOI6Vkh6RdEiRnydnZS8Q6hN/ernHzO4ys68lHZM5fGNme6d+IjKzLmb2gJl9YWYzzey0HJ5yoKQrQgifhhA+lnSlpONyzTuEsEzS7ZI6m1nbuGJ91syuMbOFkobEuZ5oZu+Y2SIze9TMNko5jn5m9m48AvB35faCOFDSTSGEt0MICyVdnM9xoDjK0K9lZt0lHS7p8nzzroB+LTNrJ+nPioodVIhS9+kQwgshhFGSZjYk7xDCCkm3SlpT0sa1OZrZuWY2V9LNca4Hmtmr8SjaJDPrkXIcO5jZdDP72szukrR6Ds//WQjhBklTG3IcxVDxBULs55JGS2oj6Z5VNTSz5pLGS3pZUmdJfSWdY2Z7xev3MLP5q9jFjyW9mrL8ahzLiZmtrugNeVYI4cs4vIuktyV1lHSZmR0i6RxJB8WxFxUdp8xsPUljFb0IdpA0R9JOKfvfOO6oG+ZwHJ2TKmSUTSn7tSRdp+jT1nf5JlwB/VqShku6VtK8fI8DRVPqPt1g8QjBCZK+ljQjDneR1FpSV0mnmlkvRYXCiZLWVVRQPGRmLeO/iYfiWPv43wenHmfcp3cu9rEUWrUUCJNCCONCCCtDCEvrabuzpHVCCMNCCMtCCB9I+qekIyQphPBMCKFD0oZmZoqqyMUp4cWS1s4h16PM7EtJH0vaWtEfTK2PQgg3hBBWxMfxa0nDQgjvhhB+UPQp/ydm1lnS/pKmhxAeCCEsl/Q3SV/U7iiEMDOE0DaE8GkdebROOA7leCworpL0a0kys0Ml/RBCGJdnrhXRry2a+9BL0j/yPA4UV8n6dAHsGvfpuYqG9w8OIXwdr/tB0tA4r6WSTpb0jxDCy3E/vzVu10vSTyUFSdeGEJaHEO6WNK32SeL2bUMILxTxWIqiVOc9G+rjHNp2k9Q1/o+v1VzShPo2DCEEM/tW0jop4XUUVZbZGh1COK6OdZnH0U3S9fEwa62ViqrXDVPbhxBWmtmcHPL4Rv44auOoDCXp12bWWtKlkvbNKbt0Ze/XZtZMUWFwRghhhWU3dxilVZI+XSCTQgh96lj3eXw6rVY3SUeb2VkpsZaKRj5aSpoTQkidMDu7oJmWSbUUCJkzlZco+qRfa/2Uf38s6f0QwpZ5PtebkraV9Eq8vG0cK4TM4/hY0nkhBDcUF5/f6pey3EzRC2y2ao/j/nh5W0mfpAwLo/xK1a+3UDRU+lz8ptpSUpv4/GqveK5NQ5SqX7dXNHH4vvg4msf7mCtpQAhhcu6po8BK+VpdTEl9+sIQwmWZDeNTIpl9uKsK975RNtVyiiHTdEn7mVk7M9tA0pkp656XtMzMzjazVvH5n63NbIcs9/0vSWeb2YYWfd3kLEkja1ea2RwzO6ZAxzFC0p/NbMt4323j87dSdG6up5kdFJ8jO0vR+dxs/UvSSWa2hZm1VzR5bGSB8kZxFKtfT1f0gtUzfvxa0qfxvz+VqqZfL1D0ia32OA6I4z0lTSlQ7iisor1WW/SV9FaSWkSL1srMWqSsn2RmQwp0HDdJOs3MelmktZkdYGZrSZokqZmZnW5mq8Wn87bPZefxcdRObFw9ntdQdtVaIIxUNClqtqTHJN1duyI+59lf0k8kzZI0X9KNiofYzawfxah0AAAfg0lEQVRPxpBWpn9IelxR9feaogkn/4y3baXoKzEvFuIgQghjFH1LYoyZfRU/377xus8VzTj/q6IXxq6pz2tmP7LoO7yJk7lCCOMlXSXpWUW/h/clXVSIvFE0I1WEfh1C+CGEMLf2IWmRpBXx8opq6dchknoc8+P43IzhYFSOkSrea/WekpZKeljSj+J/P5qyvouk5wpxECGEFxV9Bf4GRX8/70k6Jl73vaI5OSfF6wZIerB227jw+cbMeiftOy6Ul0qqPdYPFI28lJ2lnzbBqphZH0knhBCOLXcuQKHQr9HYmFmNpDtCCLuVOZWqRoEAAACcaj3FAAAAiogCAQAAOBQIAADAyek6CB06dAg1NTVFSgXVburUqfNDCLl8FbPs6NNYFfo0Gptc+nROBUJNTY2mTOHrxkhmZlV39TD6NFaFPo3GJpc+zSkGAADgUCAAAACHAgEAADgUCAAAwKFAAAAADgUCAABwKBAAAIBDgQAAABwKBAAA4FAgAAAAhwIBAAA4FAgAAMChQAAAAA4FAgAAcCgQAACAQ4EAAAAcCgQAAOCsVu4EKtGSJUtc7JxzznGxESNGuNiOO+7oYmPGjHGxbt265ZkdAADFxwgCAABwKBAAAIBDgQAAABwKBAAA4DBJMcGnn37qYjfffLOLNW/e3MWmTJniYuPGjXOx008/Pc/sgHSvvPJK2vKAAQNcm1mzZpUom1V74oknXGzLLbd0sY022qgU6QD/lfQ6feCBB6YtX3vtta7NoEGDXCzpvaEaMYIAAAAcCgQAAOBQIAAAAIcCAQAAOE1+kuIXX3zhYgMHDixDJkB+Hn/88bTl77//vkyZ1O/hhx92sVtvvdXF7r777lKkgyZqwYIFLpY02TDTGWec4WInnHCCi62xxhr5JVZhGEEAAAAOBQIAAHAoEAAAgEOBAAAAnCY3SfGaa65JW37wwQddm5dffrmgzzlx4kQXCyGkLW+77bauze67717QPFD9fvjhBxd75JFHypBJfpJuh37llVe6WNIt19daa62i5ISm59lnn3WxTz75pN7tjjzySBdr1apVQXKqRIwgAAAAhwIBAAA4FAgAAMChQAAAAE6Tm6T429/+Nm25FLflvP/+++uNde3a1bW59957XWyHHXYoXGKoOv/5z39cbPLkyWnLf/jDH0qVTs4WLlzoYm+++aaLffvtty7GJEXkI+nKohdffHFe+zr22GNdzMzy2lc1YAQBAAA4FAgAAMChQAAAAA4FAgAAcBr1JMX+/fu7WOYVDFesWFHQ5+zQoYOLJU2umj17dtryzJkzXZtevXq52MqVKxuQHarJ66+/7mJHHHGEi3Xv3j1t+dxzzy1aTg2VdLtnoJhee+01F3vllVey2na11dLfIn/2s58VJKdqwQgCAABwKBAAAIBDgQAAABwKBAAA4DSaSYrPPPOMi73zzjsulnnVq4ZcSfGUU05xsX322cfF2rRp42JPP/102vIll1yS1XPecMMNLjZo0KCstkV1SeoTSVcYHDVqVNpy69ati5ZTLpKumpj0d9qYr0SH8ku6km22+vbtW8BMqg8jCAAAwKFAAAAADgUCAABwqnIOwqxZs1ws6QIy8+fPz2v/SXdWPOSQQ1zsggsucLE111wzq+fo1q1b2vKNN97o2iTlP3jwYBf77rvvXOz00093sRYtWmSVG0pv7NixLvbII4+4WOZFkaTkC2pVgqQ75iXNN+jTp4+LtW3bthgpoQlKmveSpGXLli42bNiwQqdTVRhBAAAADgUCAABwKBAAAIBDgQAAAJyqnKS4fPlyF8t3QuLuu+/uYvfcc4+LJd2lsSEyJykm3YHvd7/7nYstWbLExZImLh544IEutskmm+SSIkpozJgxLpb0f13JF8XKnDw8evRo1ybz7niSNGTIEBdjQi3yMXnyZBd7/vnns9o2aYJ5z549G5xTNWMEAQAAOBQIAADAoUAAAAAOBQIAAHCqcpJiQ2Rede62225zbQo9ITEbSZMK77zzThd76aWXSpEOimjx4sUu9sILL2S17amnnlrodArmpptuSlv+4osvXJutttrKxfbcc8+i5YSm5eWXX85720qeAFwujCAAAACHAgEAADgUCAAAwKFAAAAATqOZpLhixYqs2r344otFziQ/IQQXW7lyZVbtko496VbUo0aNyjM7FNL333/vYnPmzHGxI488shTpFMyMGTPqbdOjR48SZIKmKttJikm3E6/kCcDlwggCAABwKBAAAIBDgQAAABwKBAAA4FTlJMURI0a4WPPmzcuQSeGMGzfOxaZNm+ZiZuZiScd+4YUXFiYxFNzaa6/tYkm3lX399dddbOHChS7Wvn37wiSWg3nz5rlY0i2rM/30pz8tRjpogiZNmuRiSbcYT9KmTRsX69KlS4NzamwYQQAAAA4FAgAAcCgQAACAQ4EAAACcqpykOH78+HKnkJOk296+9dZbacvDhg3Le/9Jt6du0aJF3vtDca2xxhou1r17dxcbO3asi+23334u9rvf/a4wiUl64403XCzpComzZ892saQJtJmaNeMzCQpjwYIFLpZ0pdkkffv2LXQ6jRJ/rQAAwKFAAAAADgUCAABwKBAAAIBTlZMUq80ll1ziYtdff31e+6qpqXGx22+/3cW6du2a1/5RHkOHDnWxpAlXSRN0jzjiiILl0bFjRxdLmnw4f/78vPZ//PHH57UdkCmbK3dKybd2PvnkkwudTqPECAIAAHAoEAAAgEOBAAAAHOYgFFj//v1d7J133inY/rfaaisX22233Qq2f5THlltu6WL33nuviyXd4TPpQkb5OuSQQ7JqN3DgQBcbNWpUvdslXSQKqM+cOXNcLNs7NybdpbFXr14NzqkpYAQBAAA4FAgAAMChQAAAAA4FAgAAcKpykmLSBWRWrFiR1baPPvpovW1OOukkF/v000+z2n9Sbtnc5S5b1XYnSxTWdtttl1Ws2H70ox/ltd3rr7/uYltvvXVD00EjN3nyZBfL9s6NBx10UKHTaTIYQQAAAA4FAgAAcCgQAACAQ4EAAACcqpykOGjQIBcbPHhwVtvut99+acvNmzfParts2yVNlsx220ynnHJKXtsBxZY0QSybSWNMSEQ+FixYkFW7Dh06uNhvf/vbQqfTZDCCAAAAHAoEAADgUCAAAACHAgEAADhVOUlxwIABLnb55Ze72Pz580uRTr2SJs5k3t735ptvdm022GCDouUENETS1UELecVQINXjjz+eVbuNNtrIxdq0aVPodJoMRhAAAIBDgQAAABwKBAAA4FAgAAAApyonKXbr1s3F7rnnHhd78MEHXezqq68uSk6r8uc//9nFTj/99JLnARTKd999V2+bNdZYowSZoDFavnx52vIHH3yQ1XatWrVysRYtWhQkp6aIEQQAAOBQIAAAAIcCAQAAOBQIAADAqcpJikl23333rGL77LNP2vJNN93k2owbN87FDjjgABf79a9/7WJJt7zdaqutXAyoZrfddpuLtW3bNm35/PPPL1U6aGSaNUv/7NqrVy/X5s0333SxTTfdtGg5NUWMIAAAAIcCAQAAOBQIAADAoUAAAABOo5mkmK1+/fqtchlA/ZImjZ111llpy3vuuWep0kEj07x587TlSy65xLVJur349ttvX7ScmiJGEAAAgEOBAAAAHAoEAADgNLk5CAAaLuliYkCxbLjhhi526623liGTpoURBAAA4FAgAAAAhwIBAAA4FAgAAMChQAAAAA4FAgAAcCgQAACAQ4EAAAAcCgQAAOBQIAAAAIcCAQAAOBQIAADAoUAAAAAOBQIAAHAshJB9Y7MvJM0uXjqoct1CCB3LnUQu6NOoB30ajU3WfTqnAgEAADQNnGIAAAAOBQIAAHAoEAAAgEOBAAAAHAoEAADgUCAAAACHAgEAADgUCAAAwKFAAAAADgUCAABwKBAAAIBDgQAAABwKBAAA4FAgAAAAhwIBAAA4FAgAAMChQAAAAA4FAgAAcCgQAACAQ4EAAAAcCgQAAOBQIAAAAIcCAQAAOBQIVcTMJpnZceXOAygUMxtlZkPLnQdQSI3ltbrsBYKZfZPyWGlmS1OWjy5jXqub2ftmNiuHbS42s+Vx7l+a2XNmtlMR08w2r2fMLJQ7j6ai0vq0mbUzszvM7Aszm2dm5+Ww7YlmtiLO/Sszm2Zm/YuZ7ypyMTM738w+inMZbWaty5FLU1SB/Tr19bb20TWPbcv+Wm1m3c3sETP72szmm9mwcuWSquwFQgihde1D0keSDkiJ3ZnZ3sxWK1Fqf5T0WR7b3Rkfy3qSXpR0X1KjUh2HmQ2UZKV4LkQqsE9fI6mFpK6Sdpb0KzM7NoftJ8bH0k7SvySNMbM2mY1KcBy/knSEpN6SOktaR9Lfi/yciFVgv5bi19uUx0e5bqsyv1ab2eqS/k/S45I6SdpI0l3FfM5slb1AqE9c6d1jZneZ2deSjskcljSzvVM/6ZtZFzN7IP7ENNPMTsvxObtLOlzS5fnmHUJYJul2SZ3NrG38SexZM7vGzBZKGhI/14lm9o6ZLTKzR81so5Q8+pnZu2a22Mz+rhzf6M2snaQ/Kyp2UCHK0Kf3l3RZCGFpCOFDSbcperPNSQhhhaRbJa0paePaHM3sXDObK+nmONcDzezV+JPZJDPrkXIcO5jZ9PiT0l2SVs8hhQMk3RxC+CSE8LWiv88jzaxVrseCwivHa3UhVMBr9QmSZoUQ/h5C+Db+O329kMeYr4ovEGI/lzRaUhtJ96yqoZk1lzRe0suKPmX0lXSOme0Vr9/DzObX83zXSfqDpO/yTTiuCo9T9B//ZRzeRdLbkjpKuszMDpF0jqSD4tiLio5TZraepLGK3tw7SJojaaeU/W8cvwBvuIo0hku6VtK8fI8DRVPKPm1Kf8EyST3qaLuqPFZT9GL2taQZcbiLpNaKRidONbNeigqFEyWtq6igeMjMWsZ/Ew/Fsfbxvw9OPc64T++cw3GsIWmTXI8FRVPq1+qDzWyhmb1hZr/OJ+EKeK3eWdJHZva4RacXnjazH+dzLIVWLQXCpBDCuBDCyhDC0nra7ixpnRDCsBDCshDCB5L+qWhoUiGEZ0IIHera2MwOlfRDCGFcnrkeZWZfSvpY0taK/mBqfRRCuCGEsCI+jl9LGhZCeDeE8IOkiyX9xMw6K/rUNz2E8EAIYbmkv0n6onZHIYSZIYS2IYRP6ziOnST1kvSPPI8DxVWyPi3pMUl/NLPWZrapohfDNXPIdde4T8+VdIikg+NP8JL0g6ShcV5LJZ0s6R8hhJfjfn5r3K6XpJ9KCpKuDSEsDyHcLWla7ZPE7duGEF6oI49HJZ1sZt3MrK2kwXE8l2NBcZWyX98laUtFb9inSLoofv3OVkW8Visqso+Mt9tQ0emGh8ysRQ7HUhSlOp/fUB/n0LabpK7xf3yt5pIm1LehRROeLpW0b07ZpRsdQjiujnWZx9FN0vXxkFStlYo6zIap7UMIK81sTjYJmFkzRYXBGSGEFWZMQahAJenTsdMVjSTNUPTCdZekX+Tw/JNCCH3qWPd5PESbmuvRZnZWSqylok+ILSXNCSGkTpidnUMeNyv623hW0YebqyT1V/SJDZWhZP06hPBmyuIkM7tWUQE7JsvnL/trdWyppGdCCE9Ikpldpui0xmaS3lzVhsVWLQVC5gz8JUr/1LB+yr8/lvR+CGHLPJ5nC0VDpc/Fb6otJbWJz6/2CiHk0vmTZB7Hx5LOCyG4obj4vG2/lOVmijpjNtpL6inpvvg4msf7mCtpQAhhcu6po8BK1acVQpiv6BOKJMnMLpf0Uj77Stp9xvLHki4MIVyW2TAeOs7sw12V5YtgPAdiiP53Trh//Hxzc8wZxVOyfl3Hcxfq01CpXqsl6TVJO6ziucumWk4xZJouaT+Lvr61gaQzU9Y9L2mZmZ1tZq3i85pbm9kOybty++2q6M21p6JhpU/jf38qSWY2x8yOKdBxjJD0ZzPbMt532/hclxSdm+tpZgfF537PUjSUlo0Fij6x1R7HAXG8p6QpBcodhVWsPl37Far2Zraame2naB7BJSnrJ5nZkAIdx02STjOzXhZpbWYHmNlakiZJamZmp8e5HCpp+2x3bGYdzOxH8X57SLpC0emNinlBhVPMfn1w/Jpp8SnV0xXNa6ldXw2v1ZJ0h6LTeHvG8zJ+L+kTSe8WKPe8VWuBMFLRBJLZis6v3l27Ij4/1F/STyTNkjRf0o2KvhIlM+uTMaSl1G1DCHNrH5IWSVoRL6+waLZ0O0UTVBoshDBG0pWKvjb2laJKct943eeKvknxV0Vv+F1Tnzd+ofwmaeJLiKQex/w4PjdjOBiVY6SK0KdjvRR9Sv9K0l8kHR5CeCdlfRdJzxXiIEIIL0oaJOkGRX8/70k6Jl73vaLzvCfF6wZIerB22/gN4hsz613H7jsq+t0sUfSifGPKHAdUppEqXr8+StKHiibN3i7p4tqvW1bLa3W8/VuSBkq6RdHfRX9F83x+KETuDWEU39kzsz6STggh5PIdcqBimVmNpDtCCLuVORWgYHitLgwKBAAA4FTrKQYAAFBEFAgAAMChQAAAAE5O10Ho0KFDqKmpKVIqqHZTp06dH0LI5es9ZUefxqrQp9HY5NKncyoQampqNGUKX6NHMjPL5ap4FYE+jVWhT6OxyaVPc4oBAAA4FAgAAMChQAAAAA4FAgAAcCgQAACAQ4EAAAAcCgQAAOBQIAAAAIcCAQAAOBQIAADAoUAAAAAOBQIAAHAoEAAAgEOBAAAAHAoEAADgUCAAAACHAgEAADirlTsBAAAq0aJFi9KWP/roo7z31a1bNxe76qqrXKxHjx4uttlmm6Utb7vttnnnkQtGEAAAgEOBAAAAHAoEAADgUCAAAACnUU9SnDdvnosddthhacu77LKLa3PyySe7WE1NTcHyKrTFixe72LPPPuti/fr1c7EWLVoUJScAqFTjx493sXHjxrnYhAkT0pbff//9vJ9z8803d7FZs2a52Pfff1/vvlauXJl3HrlgBAEAADgUCAAAwKFAAAAADgUCAABwGs0kxcwrXknSj3/8YxfLnNDXqVMn16baJiRuv/32LjZ//nwXmzJliottuummhUkMFeOrr75ysT/+8Y8u9uabb7rYk08+6WJMZEUlmjFjhotdf/31LnbTTTe52NKlS10shFCYxOrw7rvvFnX/xcAIAgAAcCgQAACAQ4EAAAAcCgQAAOBU5STFpAl4mVdIlKQFCxa42GmnnZa2fO211xYusRK4+OKLXWzmzJkuljQxhwmJjdOoUaPSlocMGeLaZHub2qQJjuuuu25+iQFFNGfOHBe7+uqry5CJt8UWW7hY0m2cKx0jCAAAwKFAAAAADgUCAABwKBAAAIBTlZMUX3nlFRfLvC1nXc4///wCZ1M8b7zxhotdccUVLvbzn//cxQ4//PCi5ITySpqYddZZZ6UtJ03iNbOs9n/GGWe42HXXXedi7du3z2p/QKqkvpk0sXDXXXdNW066VX3Lli1drE2bNi7WunVrF/vmm29cbN9993WxzImFO+20k2uz3Xbbudgaa6zhYmuttZaLVTpGEAAAgEOBAAAAHAoEAADgUCAAAACn4icpzps3z8Xuu+++rLa99dZbXaxjx44NzqkYkiYk9u3bN6ttBwwY4GJrr712g3NC5UmapJp0xdB83X333S726KOPuljS1RozJzgmTSJD07FkyRIXS3pNe/XVV13swQcfrHf/vXv3drFp06a5WE1NjYslXVm0S5cuLtasWdP+DN20jx4AACSiQAAAAA4FAgAAcCgQAACAU/GTFM8++2wXy7y9rSRtv/32LnbooYcWJadimDRpkovNnTvXxY4//ngXO+aYY4qSE8pr9uzZLnbbbbfVu922227rYp06dXKx//u//8sqj8WLF7tY0mTJo48+Om15/fXXz2r/qH7Lli1zsaOOOsrFkiYknnvuuS62995755VH0oTEJF27ds1r/00NIwgAAMChQAAAAA4FAgAAcCp+DkLSXeiSYp07d3axSrlQy9KlS11s2LBhacvXX3+9a5N0nEkXf0LjNH36dBf76quvXGz33XdPW37mmWdcm++++87FRo8e7WKXXnqpi33wwQculjQ/5qCDDkpbTrrAEneBrH5Jd0LMfD2TpHHjxrlY0oXqzjnnHBdbc80188wOhcQIAgAAcCgQAACAQ4EAAAAcCgQAAOBU/CTFbI0fP97F9tlnHxdr27Zt2vKgQYMKmseECROyir3wwgv17quaLvSEwvv+++9dLGni6llnnVXvvlq1auViv/rVr1xs7NixLjZjxgwXCyG4WObEskqZJIzCSrrT4vDhw12sW7duLjZx4kQXa9OmTWESQ8ExggAAABwKBAAA4FAgAAAAhwIBAAA4FT9J8Te/+Y2LPf300y726aefuljSFeUyJ1c99NBDDcjOS5q8lTSxLNMmm2ziYklXJ0PTcdddd2XV7t///nfa8sEHH5z3c06ZMiXvbXfeeee05datW+e9L1SuyZMnZ9Vuu+22c7EuXboUOh0UESMIAADAoUAAAAAOBQIAAHAoEAAAgFPxkxR32GEHF3v99dddLOnWuI899piLXX755WnL6623nmszcODAXFJMc+yxx7rYNttsU+92u+yyi4slTVxE03HkkUe6WNKk2pdffjlt+Z133nFtkv5mHnjgARdbtGiRi2VefbSudjfddFPactLfwlZbbeViqC5JV9tMknS77wsvvNDFDjzwQBdLmuCI0mMEAQAAOBQIAADAoUAAAAAOBQIAAHAs6cp/ddlxxx1DQ6601hR8+OGHLpY02bBnz55py0888YRr07Fjx8IlVgJmNjWEsGO588hFJffphQsXulhSX1q8eHHacr5X85Skvn37utj111/vYvvvv7+Lvffee2nLJ598smszYsSIrPKoFPRpL6kvZdu/kjRv3tzFTjnllLTlnXbaybX5+OOPXax79+4u9uMf/zirPN58800X6927d9pyY7gSZC59mhEEAADgUCAAAACHAgEAADgUCAAAwKn4KylWm4suusjFkibwZF7RsdomJKL42rdv72JjxoxxsUMOOSRtOXPSopQ8cfHMM890scsuu8zFWrVq5WIDBgxwsUsvvTRt+fHHH3dtZsyY4WJcMbS6/P73v3exv/3tb3nvb8WKFS6WOTE2aaJsKWReabdPnz6uzd13312ibEqPEQQAAOBQIAAAAIcCAQAAOBQIAADAYZJiAyRNGLv99ttdbJ111nGxddddtyg5oXHbe++9XSzz9rujR492bZJu2Zw0oTZpQmKS8847z8XefvvttOWkW1MnPWfS3wwq1/Dhw13ssMMOc7Gjjz7axZYvX+5ic+bMcbGkiYvlMG/evLTlpNf8Hj16uNiQIUOKllMpMYIAAAAcCgQAAOBQIAAAAIc5CA3w6KOPZtVuv/32c7Htt9++0Omgicqcl5A0T6HQ1lhjDRc7/PDD05aT5iD85z//cbGku1YmXSQKlSHp7ou9evVyscy7e9blqaeecrHMuQpDhw51bV566aWs9l9ISRccmzp1asnzKBVGEAAAgEOBAAAAHAoEAADgUCAAAACHSYoNkDRJca211nKxpLufAY1N5sVyHn74Ydcm6c531113nYudf/75hUsMFW2vvfaqt8306dNdLGmSYosWLVzs+OOPd7GTTjrJxa666ioXS7roWFPCCAIAAHAoEAAAgEOBAAAAHAoEAADgMEkxSyNGjHCxuXPnulinTp1cjKsmoilo1iz988bgwYNdmwcffNDFkq6Sd8QRR7jYZpttln9yqGr77LOPi5177rkulnS3yJtuusnF3n//fRebMGFCXrl17tw5r+2qASMIAADAoUAAAAAOBQIAAHAoEAAAgMMkxSwlTVI0Mxfr379/Vvv7+uuv05YXLVrk2nTt2jXL7IDK07NnTxf7y1/+4mJJVxr905/+5GKjRo1ysaTbTqPx2XLLLV0s8/biknTPPfdktb+k244nWW219LfI/fbbz7W57LLLstpXNWIEAQAAOBQIAADAoUAAAAAOBQIAAHCYpFhgmZNapOTJVZm3Fu3Ro4drc/vttxcuMaAC/PKXv3SxG2+80cXuv/9+F0u6+t0222xTmMRQ0ZImo1599dUuljn5W5KmTp3qYp9//rmL1dTUuFhmf0266mdjxggCAABwKBAAAIBDgQAAABwKBAAA4DBJscBuvvlmF7vllltc7MQTT0xbPu+884qWE1ApOnbs6GJPPvmki3Xr1s3Fhg8f7mKjR48uTGKoOp06dXKx8ePHu9gdd9zhYs8//7yLJU1AXG+99fJLrpFgBAEAADgUCAAAwKFAAAAADgUCAABwmKSYpWuvvdbFLrjgAhfbfffdXWzQoEEu1q5du7Tlli1bNiA7oHol3da8b9++Lvbwww+72FtvveViW221VWESQ6Nw7LHHZhWDxwgCAABwKBAAAIBDgQAAABzmIGRpt912c7Gnn366DJkAjd/YsWNdbNttt3WxDz74wMWYgwAUBiMIAADAoUAAAAAOBQIAAHAoEAAAgMMkRQAVZ5111nGxmTNnliEToOliBAEAADgUCAAAwKFAAAAADgUCAABwKBAAAIBDgQAAABwKBAAA4FAgAAAAhwIBAAA4FkLIvrHZF5JmFy8dVLluIYSO5U4iF/Rp1IM+jcYm6z6dU4EAAACaBk4xAAAAhwIBAAA4FAgAAMChQAAAAA4FAgAAcCgQAACAQ4EAAAAcCgQAAOBQIAAAAOf/AzWYTrBupQhFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test, y_test = load_data(mode='test')\n",
    "feed_dict_test = {x: x_test[:1000].reshape((-1, timesteps, num_input)), y: y_test[:1000]}\n",
    "loss_test, acc_test = sess.run([loss, accuracy], feed_dict=feed_dict_test)\n",
    "print('-----------------------------------')\n",
    "print(\"Test loss: {0:.2f}, test accuracy:  {1:.01%}\" .format(loss_test, acc_test))\n",
    "print('-----------------------------------')\n",
    "\n",
    "# Plot some of the correct and misclassified examples\n",
    "cls_pred = sess.run(cls_prediction, feed_dict=feed_dict_test)\n",
    "cls_true = np.argmax(y_test, axis=1)\n",
    "plot_images(x_test, cls_true, cls_pred, title='Correct Examples')\n",
    "plot_example_errors(x_test[:1000],cls_true[:1000], cls_pred, title='Misclassified Examples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_static_lstm(input_x, n_steps, n_hidden):\n",
    "    input_x1 = tf.unstack(input_x, num=n_steps, axis=1)\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=n_hidden, forget_bias=1.0)\n",
    "    hiddens, states = tf.contrib.rnn.static_rnn(cell=lstm_cell, inputs=input_x1, dtype=tf.float32)\n",
    "    return hiddens,states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_static_gru(input_x, n_steps, n_hidden):\n",
    "    input_x1 = tf.unstack(input_x, num=n_steps, axis=1)\n",
    "    gru_cell = tf.contrib.rnn.GRUCell(num_units=n_hidden)\n",
    "    hiddens,states = tf.contrib.rnn.static_rnn(cell=gru_cell, inputs=input_x1, dtype=tf.float32)\n",
    "    return hiddens,states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_dynamic_lstm(input_x, n_steps, n_hidden):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell (num_units=n_hidden, forget_bias=1.0)\n",
    "    hiddens, states = tf.nn.dynamic_rnn(cell=lstm_cell, inputs=input_x, dtype=tf.float32)\n",
    "    hiddens = tf.transpose(hiddens,[1,0,2])\n",
    "    return hiddens,states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_dynamic_gru(input_x, n_steps, n_hidden):\n",
    "    gru_cell = tf.contrib.rnn.GRUCell (num_units=n_hidden)\n",
    "    hiddens, states = tf.nn.dynamic_rnn(cell=gru_cell, inputs=input_x, dtype=tf.float32)\n",
    "    hiddens = tf.transpose(hiddens,[1,0,2])\n",
    "    return hiddens,states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_static_lstm(input_x,n_steps,n_hidden):\n",
    "    input_x1 = tf.unstack(input_x,num=n_steps,axis=1)\n",
    "    stacked_rnn = []\n",
    "    for i in range(3):\n",
    "        stacked_rnn.append(tf. contrib.rnn.LSTMCell(num_units=n_hidden))\n",
    "    mcell = tf.contrib.rnn.MultiRNNCell(cells=stacked_rnn)\n",
    "    hiddens, states = tf.contrib.rnn.static_rnn(cell=mcell,inputs=input_x1,dtype=tf.float32)\n",
    "    return hiddens, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_static_gru(input_x,n_steps,n_hidden):\n",
    "    input_x1 = tf.unstack(input_x,num=n_steps,axis=1)\n",
    "    stacked_rnn = []\n",
    "    for i in range(3):\n",
    "        stacked_rnn.append(tf.contrib.rnn.GRUCell(num_units=n_hidden))\n",
    "    mcell = tf.contrib.rnn.MultiRNNCell(cells=stacked_rnn)\n",
    "    hiddens,states = tf.contrib.rnn.static_rnn(cell=mcell,inputs=input_x1,dtype=tf.float32)\n",
    "    return hiddens, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_static_mix(input_x,n_steps,n_hidden):\n",
    "    input_x1 = tf.unstack(input_x,num=n_steps,axis=1)\n",
    "    gru_cell = tf.contrib.rnn.GRUCell (num_units=n_hidden*2)\n",
    "    stm_cell = tf.contrib.rnn.LSTMCell (num_units=n_hidden)\n",
    "    mcell = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell,gru_cell])\n",
    "    hiddens,states = tf.contrib.rnn.static_rnn(cell=mcell,inputs=input_x1,dtype=tf.float32)\n",
    "    return hiddens, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_dynamic_lstm(input_x,n_steps,n_hidden):\n",
    "    stacked_rnn = []\n",
    "    for i in range(3):\n",
    "        stacked_rnn.append(tf.contrib.rnn. LSTMCell (num_units=n_hidden))\n",
    "    mcell = tf.contrib.rnn.MultiRNNCell(cells=stacked_rnn)\n",
    "    hiddens,states = tf.nn.dynamic_rnn(cell=mcell, inputs=input_x,dtype=tf.float32)\n",
    "    hiddens = tf.transpose(hiddens,[1,0,2])\n",
    "    return hiddens, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_dynamic_gru(input_x,n_steps,n_hidden):\n",
    "    stacked_rnn = []\n",
    "    for i in range(3):\n",
    "        stacked_rnn.append(tf.contrib.rnn.GRUCell(num_units=n_hidden))\n",
    "    mcell = tf.contrib.rnn.MultiRNNCell(cells=stacked_rnn)\n",
    "    hiddens,states = tf.nn.dynamic_rnn(cell=mcell, inputs=input_x,dtype=tf .float32)\n",
    "    hiddens = tf.transpose(hiddens,[1,0,2])\n",
    "    return hiddens, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_dynamic_mix(input_x,n_steps,n_hidden):\n",
    "    gru_cell = tf.contrib.rnn.GRUCell(num_units=n_hidden*2)\n",
    "    lstm_cell = tf.contrib.rnn.LSTMCell(num_units=n_hidden)\n",
    "    mcell = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell,gru_cell])\n",
    "    hiddens, states = tf.nn.dynamic_rnn(cell=mcell, inputs=input_x,dtype=tf.float32)\n",
    "    hiddens = tf.transpose(hiddens,[1,0,2])\n",
    "    return hiddens, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_static_bi_lstm(input_x,n_steps,n_hidden):\n",
    "    input_x1= tf.unstack(input_x,num=n_steps, axis=1)\n",
    "    lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell (num_units=n_hidden, forget_bias = 1.0) \n",
    "    lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell (num_units=n_hidden, forget_bias = 1.0) \n",
    "    hiddens,fw_state,bw_state = tf.contrib.rnn.static_bidirectional_rnn(cell_fw=lstm_fw_cell,cell_bw=lstm_bw_cell,\n",
    "                                                                        inputs=input_x1,dtype=tf.float32)\n",
    "    print(' hiddens: In', type (hiddens), len(hiddens),hiddens [1].shape, hiddens[1].shape)\n",
    "    return hiddens,fw_state,bw_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_dynamic_bi_lstm(input_x,n_steps,n_hidden):\n",
    "    lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell (num_units=n_hidden, forget_bias = 1.0)\n",
    "    lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell (num_units=n_hidden, forget_bias = 1.0)\n",
    "    hiddens,state= tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell,cell_bw=lstm_bw_cell,\n",
    "                                                   inputs=input_x,dtype=tf.float32)\n",
    "    print(' hiddens:\\ n',type(hiddens), len(hiddens),hiddens[0].shape,hiddens[1].shape)\n",
    "    hiddens=tf.concat(hiddens, axis=2) \n",
    "    hiddens=tf.transpose(hiddens, [1,0,2])\n",
    "    return hiddens,state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_static_bi_lstm(input_x,n_steps,n_hidden):\n",
    "    input_x1=tf.unstack(input_x, num=n_steps,axis=1)\n",
    "    stacked_fw_rnn = []\n",
    "    stacked_bw_rnn = []\n",
    "    for i in range(3):\n",
    "        stacked_fw_rnn.append(tf.contrib.rnn.BasicLSTMCell(num_units=n_hidden, forget_bias=1.0))\n",
    "        stacked_bw_rnn.append(tf.contrib.rnn.BasicLSTMCell(num_units=n_hidden, forget_bias=1.0))\n",
    "    hiddens,fw_state,bw_state =tf.contrib.rnn.stack_bidirectional_rnn(stacked_fw_rnn,stacked_bw_rnn,\n",
    "                                                                        inputs=input_x1,dtype=tf.float32)\n",
    "    print( 'hiddens:\\n',type(hiddens),len(hiddens),hiddens[0].shape,hiddens[1].shape)\n",
    "    return hiddens,fw_state,bw_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_dynamic_bi_lstm(input_x,n_steps,n_hidden):\n",
    "    stacked_fw_rnn = []\n",
    "    stacked_bw_rnn = []\n",
    "    for i in range(3):\n",
    "        stacked_fw_rnn.append(tf.contrib.rnn.BasicLSTMCell (num_units=n_hidden, forget_bias = 1.0))\n",
    "        stacked_bw_rnn.append(tf.contrib.rnn.BasicLSTMCell (num_units=n_hidden, forget_bias = 1.0))\n",
    "    tf.contrib.rnn.MultiRNNCell\n",
    "    hiddens,fw_state,bw_state = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(stacked_fw_rnn, stacked_bw_rnn,\n",
    "                                                                               inputs=input_x,dtype=tf.float32)\n",
    "    print(' hiddens: \\n' , type (hiddens), hiddens.shape)\n",
    "    hiddens = tf.transpose(hiddens, [1,0,2])\n",
    "    return hiddens,fw_state,bw_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_rnn_classfication(flag):\n",
    "    '''\n",
    "    1.\n",
    "    '''\n",
    "    tf.reset_default_graph()\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    \n",
    "\n",
    "    mnist = input_data.read_data_sets('MNIST-data',one_hot=True)\n",
    "\n",
    "    print (type(mnist)) #iclass  'tensorfLow. contrib. Learn.python. Learn.datasets. base. Datasets'x\n",
    "    print('Training data shape: ' ,mnist.train.images.shape) #Training data shape: (55000,784)\n",
    "    print( 'Test data shape: ',mnist.test.images.shape) #Test data shape: (10000, )784)\n",
    "    print( 'Validation data shape:',mnist.validation.images.shape) #Validation  data shape: (5000,)784)\n",
    "    print( 'Training label shape: ',mnist.train.labels.shape)\n",
    "    '''\n",
    "    2 \n",
    "    '''\n",
    "    n_input = 28\n",
    "    n_steps = 28\n",
    "    n_hidden = 128\n",
    "    n_classes = 10\n",
    "    batch_size = 128\n",
    "    training_step = 5000\n",
    "    display_step = 200\n",
    "    learning_rate =1e-4\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    input_x=tf.placeholder(dtype=tf.float32,shape=[None,n_steps,n_input])\n",
    "    input_y=tf.placeholder(dtype=tf.float32,shape=[None,n_classes])\n",
    "    \n",
    "    if flag == 1:\n",
    "        print('LSTM:')\n",
    "        hiddens, states = single_layer_static_lstm(input_x,n_steps,n_hidden)\n",
    "    elif flag == 2:\n",
    "        print(' gru :')\n",
    "        hiddens, states = single_layer_static_gru(input_x,n_steps,n_hidden)\n",
    "    elif flag == 3:\n",
    "        print(' LSTM :')\n",
    "        hiddens, states = single_layer_dynamic_lstm(input_x,n_steps,n_hidden)\n",
    "    elif flag == 4:\n",
    "        print(' gru :')\n",
    "        hiddens, states = single_layer_dynamic_gru(input_x,n_steps,n_hidden)\n",
    "    elif flag == 5:\n",
    "        print (' LSTM :')\n",
    "        hiddens, states = multi_layer_static_lstm(input_x,n_steps,n_hidden)\n",
    "    elif flag == 6:\n",
    "        print(' gru :')\n",
    "        hiddens, states = multi_layer_static_gru(input_x,n_steps,n_hidden)\n",
    "    elif flag == 7:\n",
    "        print (' LSTM  gru :')\n",
    "        hiddens, states = multi_layer_static_mix(input_x,n_steps,n_hidden)\n",
    "    elif flag == 8:\n",
    "        print(' LSTM :')\n",
    "        hiddens, states = multi_layer_dynamic_lstm(input_x,n_steps, n_hidden)\n",
    "    elif flag == 9:\n",
    "        print(' gru :')\n",
    "        hiddens, states = multi_layer_dynamic_gru (input_x,n_steps,n_hidden)\n",
    "    elif flag ==10:\n",
    "        print('LSTMgru:')\n",
    "        hiddens, states = multi_layer_dynamic_mix( input_x,n_steps,n_hidden)\n",
    "    elif flag == 11:\n",
    "        print(' LSTM :')\n",
    "        hiddens, fw_state, bw_state = single_layer_static_bi_lstm(input_x,n_steps,n_hidden)\n",
    "    elif flag == 12:\n",
    "        print(' LSTM :')\n",
    "        hiddens, bw_state = single_layer_dynamic_bi_lstm(input_x,n_steps,n_hidden)\n",
    "    elif flag == 13:\n",
    "        print(' LSTM :')\n",
    "        hiddens, fw_state, bw_state = multi_layer_static_bi_lstm(input_x,n_steps,n_hidden)\n",
    "    elif flag == 14:\n",
    "        print(' LSTM :')\n",
    "        hiddens, fw_state, bow_state = multi_layer_dynamic_bi_lstm(input_x,n_steps,n_hidden)\n",
    "    \n",
    "    print ( 'hidden: ',hiddens[-1].shape) #(128,128)\n",
    "#LSTM,\n",
    "    output = tf.contrib. layers.fully_connected (inputs=hiddens[-1], num_outputs=n_classes, activation_fn = tf.nn.softmax)\n",
    "    '''\n",
    "    3 \n",
    "    '''\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(input_y*tf.log(output),axis=1))\n",
    "    '''\n",
    "    4 \n",
    "    '''\n",
    "    train = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    correct = tf.equal(tf.argmax(output,1),tf.argmax(input_y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "    \n",
    "    test_accuracy_list = []\n",
    "    test_cost_list = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(training_step):\n",
    "            x_batch,y_batch = mnist.train.next_batch(batch_size = batch_size)\n",
    "            x_batch = x_batch.reshape([-1,n_steps,n_input])\n",
    "            train.run(feed_dict={input_x:x_batch, input_y:y_batch})\n",
    "            if (i+1) % display_step == 0:\n",
    "                training_accuracy,training_cost = sess.run([accuracy,cost],feed_dict={input_x:x_batch, input_y:y_batch})\n",
    "                print('Step {0}:Training set accuracy {1},cost {2}.'.format(i+1,training_accuracy,training_cost))\n",
    "        \n",
    "        for i in range(200):\n",
    "            x_batch,y_batch = mnist.test.next_batch(batch_size = 50)\n",
    "            \n",
    "            x_batch = x_batch.reshape([-1,n_steps,n_input])\n",
    "            test_accuracy,test_cost = sess.run([accuracy,cost],feed_dict={input_x:x_batch, input_y:y_batch})\n",
    "            test_accuracy_list.append(test_accuracy)\n",
    "            test_cost_list.append(test_cost)\n",
    "            if (i+1) % 20 == 0:\n",
    "                print('Step {0}:Test set acuracy {1},cost {2}.'.format(i+1,test_accuracy,test_cost))\n",
    "        print('Test accuracy:',np.mean(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n",
      "Training data shape:  (55000, 784)\n",
      "Test data shape:  (10000, 784)\n",
      "Validation data shape: (5000, 784)\n",
      "Training label shape:  (55000, 10)\n",
      "LSTM:\n",
      "hidden:  (?, 128)\n",
      "Step 200:Training set accuracy 0.578125,cost 1.2835853099822998.\n",
      "Step 400:Training set accuracy 0.7578125,cost 0.7488283514976501.\n",
      "Step 600:Training set accuracy 0.8359375,cost 0.5651132464408875.\n",
      "Step 800:Training set accuracy 0.8828125,cost 0.4864320755004883.\n",
      "Step 1000:Training set accuracy 0.90625,cost 0.31166180968284607.\n",
      "Step 1200:Training set accuracy 0.9140625,cost 0.2915381193161011.\n",
      "Step 1400:Training set accuracy 0.921875,cost 0.2629339098930359.\n",
      "Step 1600:Training set accuracy 0.9375,cost 0.23520667850971222.\n",
      "Step 1800:Training set accuracy 0.9296875,cost 0.20752355456352234.\n",
      "Step 2000:Training set accuracy 0.9140625,cost 0.2710161507129669.\n",
      "Step 2200:Training set accuracy 0.9609375,cost 0.12336263805627823.\n",
      "Step 2400:Training set accuracy 0.921875,cost 0.233933225274086.\n",
      "Step 2600:Training set accuracy 0.9296875,cost 0.15949206054210663.\n",
      "Step 2800:Training set accuracy 0.9609375,cost 0.11678601801395416.\n",
      "Step 3000:Training set accuracy 0.9296875,cost 0.2101449966430664.\n",
      "Step 3200:Training set accuracy 0.96875,cost 0.09881885349750519.\n",
      "Step 3400:Training set accuracy 0.953125,cost 0.12125533819198608.\n",
      "Step 3600:Training set accuracy 0.9765625,cost 0.10570986568927765.\n",
      "Step 3800:Training set accuracy 0.9453125,cost 0.16502949595451355.\n",
      "Step 4000:Training set accuracy 0.9453125,cost 0.1728740632534027.\n",
      "Step 4200:Training set accuracy 0.9609375,cost 0.11654253304004669.\n",
      "Step 4400:Training set accuracy 0.953125,cost 0.140864759683609.\n",
      "Step 4600:Training set accuracy 0.953125,cost 0.12545858323574066.\n",
      "Step 4800:Training set accuracy 0.9765625,cost 0.11952851712703705.\n",
      "Step 5000:Training set accuracy 0.953125,cost 0.13729798793792725.\n",
      "Step 20:Test set acuracy 0.9599999785423279,cost 0.14600129425525665.\n",
      "Step 40:Test set acuracy 0.9399999976158142,cost 0.1055670753121376.\n",
      "Step 60:Test set acuracy 0.9800000190734863,cost 0.06664101779460907.\n",
      "Step 80:Test set acuracy 0.9599999785423279,cost 0.08494844287633896.\n",
      "Step 100:Test set acuracy 0.9800000190734863,cost 0.09286873787641525.\n",
      "Step 120:Test set acuracy 1.0,cost 0.020447779446840286.\n",
      "Step 140:Test set acuracy 0.9599999785423279,cost 0.13244031369686127.\n",
      "Step 160:Test set acuracy 0.9599999785423279,cost 0.16386501491069794.\n",
      "Step 180:Test set acuracy 0.9399999976158142,cost 0.1781146228313446.\n",
      "Step 200:Test set acuracy 0.9800000190734863,cost 0.0626591145992279.\n",
      "Test accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "mnist_rnn_classfication(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n",
      "Training data shape:  (55000, 784)\n",
      "Test data shape:  (10000, 784)\n",
      "Validation data shape: (5000, 784)\n",
      "Training label shape:  (55000, 10)\n",
      " gru :\n",
      "hidden:  (?, 128)\n",
      "Step 200:Training set accuracy 0.4375,cost 1.6932766437530518.\n",
      "Step 400:Training set accuracy 0.78125,cost 0.7541613578796387.\n",
      "Step 600:Training set accuracy 0.78125,cost 0.6852806806564331.\n",
      "Step 800:Training set accuracy 0.90625,cost 0.40959808230400085.\n",
      "Step 1000:Training set accuracy 0.8203125,cost 0.4501388967037201.\n",
      "Step 1200:Training set accuracy 0.890625,cost 0.3659282922744751.\n",
      "Step 1400:Training set accuracy 0.8828125,cost 0.3502284288406372.\n",
      "Step 1600:Training set accuracy 0.9296875,cost 0.23786523938179016.\n",
      "Step 1800:Training set accuracy 0.9609375,cost 0.17486904561519623.\n",
      "Step 2000:Training set accuracy 0.9140625,cost 0.22578978538513184.\n",
      "Step 2200:Training set accuracy 0.953125,cost 0.17888198792934418.\n",
      "Step 2400:Training set accuracy 0.9921875,cost 0.09048976004123688.\n",
      "Step 2600:Training set accuracy 0.96875,cost 0.17488223314285278.\n",
      "Step 2800:Training set accuracy 0.9296875,cost 0.16994476318359375.\n",
      "Step 3000:Training set accuracy 0.9375,cost 0.17615722119808197.\n",
      "Step 3200:Training set accuracy 0.9609375,cost 0.14536751806735992.\n",
      "Step 3400:Training set accuracy 0.96875,cost 0.15485183894634247.\n",
      "Step 3600:Training set accuracy 0.96875,cost 0.1740182340145111.\n",
      "Step 3800:Training set accuracy 0.96875,cost 0.09144333750009537.\n",
      "Step 4000:Training set accuracy 0.953125,cost 0.14053761959075928.\n",
      "Step 4200:Training set accuracy 0.9609375,cost 0.12088200449943542.\n",
      "Step 4400:Training set accuracy 0.953125,cost 0.20201708376407623.\n",
      "Step 4600:Training set accuracy 0.96875,cost 0.14953777194023132.\n",
      "Step 4800:Training set accuracy 0.9765625,cost 0.06666488200426102.\n",
      "Step 5000:Training set accuracy 0.96875,cost 0.08615729212760925.\n",
      "Step 20:Test set acuracy 1.0,cost 0.01526297815144062.\n",
      "Step 40:Test set acuracy 0.9200000166893005,cost 0.27367034554481506.\n",
      "Step 60:Test set acuracy 0.9599999785423279,cost 0.08907297253608704.\n",
      "Step 80:Test set acuracy 0.8799999952316284,cost 0.3106863796710968.\n",
      "Step 100:Test set acuracy 0.9200000166893005,cost 0.21704018115997314.\n",
      "Step 120:Test set acuracy 0.9599999785423279,cost 0.11099584400653839.\n",
      "Step 140:Test set acuracy 0.9800000190734863,cost 0.06398220360279083.\n",
      "Step 160:Test set acuracy 0.9800000190734863,cost 0.05868300795555115.\n",
      "Step 180:Test set acuracy 0.9599999785423279,cost 0.10793126374483109.\n",
      "Step 200:Test set acuracy 0.9399999976158142,cost 0.34795573353767395.\n",
      "Test accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "mnist_rnn_classfication(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n",
      "Training data shape:  (55000, 784)\n",
      "Test data shape:  (10000, 784)\n",
      "Validation data shape: (5000, 784)\n",
      "Training label shape:  (55000, 10)\n",
      " LSTM :\n",
      "hidden:  (?, 128)\n",
      "Step 200:Training set accuracy 0.546875,cost 1.3964611291885376.\n",
      "Step 400:Training set accuracy 0.7109375,cost 0.7756103277206421.\n",
      "Step 600:Training set accuracy 0.8359375,cost 0.5387505292892456.\n",
      "Step 800:Training set accuracy 0.8984375,cost 0.4233919382095337.\n",
      "Step 1000:Training set accuracy 0.875,cost 0.36482661962509155.\n",
      "Step 1200:Training set accuracy 0.9140625,cost 0.3286043107509613.\n",
      "Step 1400:Training set accuracy 0.890625,cost 0.3517189621925354.\n",
      "Step 1600:Training set accuracy 0.9140625,cost 0.2896823585033417.\n",
      "Step 1800:Training set accuracy 0.90625,cost 0.263533353805542.\n",
      "Step 2000:Training set accuracy 0.9609375,cost 0.12408551573753357.\n",
      "Step 2200:Training set accuracy 0.96875,cost 0.13170190155506134.\n",
      "Step 2400:Training set accuracy 0.9375,cost 0.18174883723258972.\n",
      "Step 2600:Training set accuracy 0.9296875,cost 0.2297918200492859.\n",
      "Step 2800:Training set accuracy 0.953125,cost 0.14413154125213623.\n",
      "Step 3000:Training set accuracy 0.9296875,cost 0.19704332947731018.\n",
      "Step 3200:Training set accuracy 0.953125,cost 0.20911680161952972.\n",
      "Step 3400:Training set accuracy 0.9453125,cost 0.1753774732351303.\n",
      "Step 3600:Training set accuracy 0.9296875,cost 0.23981353640556335.\n",
      "Step 3800:Training set accuracy 0.9609375,cost 0.15102219581604004.\n",
      "Step 4000:Training set accuracy 0.96875,cost 0.11200082302093506.\n",
      "Step 4200:Training set accuracy 0.96875,cost 0.11071892082691193.\n",
      "Step 4400:Training set accuracy 0.96875,cost 0.15565362572669983.\n",
      "Step 4600:Training set accuracy 0.984375,cost 0.05926127731800079.\n",
      "Step 4800:Training set accuracy 0.9765625,cost 0.05558602884411812.\n",
      "Step 5000:Training set accuracy 0.953125,cost 0.11200372874736786.\n",
      "Step 20:Test set acuracy 0.9800000190734863,cost 0.06270051002502441.\n",
      "Step 40:Test set acuracy 0.9399999976158142,cost 0.18359977006912231.\n",
      "Step 60:Test set acuracy 0.9800000190734863,cost 0.03632192313671112.\n",
      "Step 80:Test set acuracy 0.9599999785423279,cost 0.09818632155656815.\n",
      "Step 100:Test set acuracy 0.9399999976158142,cost 0.16688984632492065.\n",
      "Step 120:Test set acuracy 0.9800000190734863,cost 0.047491494566202164.\n",
      "Step 140:Test set acuracy 0.9800000190734863,cost 0.053890686482191086.\n",
      "Step 160:Test set acuracy 0.9399999976158142,cost 0.10748228430747986.\n",
      "Step 180:Test set acuracy 0.9200000166893005,cost 0.19700171053409576.\n",
      "Step 200:Test set acuracy 0.9399999976158142,cost 0.13771654665470123.\n",
      "Test accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "mnist_rnn_classfication(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n",
      "Training data shape:  (55000, 784)\n",
      "Test data shape:  (10000, 784)\n",
      "Validation data shape: (5000, 784)\n",
      "Training label shape:  (55000, 10)\n",
      " gru :\n",
      "hidden:  (?, 128)\n",
      "Step 200:Training set accuracy 0.53125,cost 1.5748648643493652.\n",
      "Step 400:Training set accuracy 0.671875,cost 1.042257308959961.\n",
      "Step 600:Training set accuracy 0.8125,cost 0.6702122688293457.\n",
      "Step 800:Training set accuracy 0.8828125,cost 0.44902896881103516.\n",
      "Step 1000:Training set accuracy 0.875,cost 0.445321261882782.\n",
      "Step 1200:Training set accuracy 0.9375,cost 0.3076552450656891.\n",
      "Step 1400:Training set accuracy 0.9140625,cost 0.3012853264808655.\n",
      "Step 1600:Training set accuracy 0.9140625,cost 0.23912334442138672.\n",
      "Step 1800:Training set accuracy 0.953125,cost 0.19981727004051208.\n",
      "Step 2000:Training set accuracy 0.9140625,cost 0.23073837161064148.\n",
      "Step 2200:Training set accuracy 0.9453125,cost 0.17320893704891205.\n",
      "Step 2400:Training set accuracy 0.90625,cost 0.28685224056243896.\n",
      "Step 2600:Training set accuracy 0.9296875,cost 0.217891663312912.\n",
      "Step 2800:Training set accuracy 0.9765625,cost 0.09883393347263336.\n",
      "Step 3000:Training set accuracy 0.9375,cost 0.144429549574852.\n",
      "Step 3200:Training set accuracy 0.9453125,cost 0.1904180645942688.\n",
      "Step 3400:Training set accuracy 0.96875,cost 0.13733623921871185.\n",
      "Step 3600:Training set accuracy 0.9453125,cost 0.15840905904769897.\n",
      "Step 3800:Training set accuracy 0.96875,cost 0.0760742574930191.\n",
      "Step 4000:Training set accuracy 0.9765625,cost 0.07447154819965363.\n",
      "Step 4200:Training set accuracy 0.9296875,cost 0.1380319595336914.\n",
      "Step 4400:Training set accuracy 0.96875,cost 0.08358461409807205.\n",
      "Step 4600:Training set accuracy 0.984375,cost 0.0600067600607872.\n",
      "Step 4800:Training set accuracy 0.9296875,cost 0.19624167680740356.\n",
      "Step 5000:Training set accuracy 0.9609375,cost 0.12165389955043793.\n",
      "Step 20:Test set acuracy 0.8999999761581421,cost 0.20192107558250427.\n",
      "Step 40:Test set acuracy 0.9800000190734863,cost 0.06514669209718704.\n",
      "Step 60:Test set acuracy 0.9599999785423279,cost 0.17535018920898438.\n",
      "Step 80:Test set acuracy 0.9800000190734863,cost 0.09341517090797424.\n",
      "Step 100:Test set acuracy 0.9399999976158142,cost 0.1238185241818428.\n",
      "Step 120:Test set acuracy 0.9599999785423279,cost 0.08975475281476974.\n",
      "Step 140:Test set acuracy 1.0,cost 0.027346152812242508.\n",
      "Step 160:Test set acuracy 0.9200000166893005,cost 0.34120142459869385.\n",
      "Step 180:Test set acuracy 0.9599999785423279,cost 0.15136438608169556.\n",
      "Step 200:Test set acuracy 0.9200000166893005,cost 0.2626856565475464.\n",
      "Test accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "mnist_rnn_classfication(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n",
      "Training data shape:  (55000, 784)\n",
      "Test data shape:  (10000, 784)\n",
      "Validation data shape: (5000, 784)\n",
      "Training label shape:  (55000, 10)\n",
      " LSTM :\n",
      "hidden:  (?, 128)\n",
      "Step 200:Training set accuracy 0.7578125,cost 0.815491259098053.\n",
      "Step 400:Training set accuracy 0.828125,cost 0.603458046913147.\n",
      "Step 600:Training set accuracy 0.8671875,cost 0.4645434617996216.\n",
      "Step 800:Training set accuracy 0.8828125,cost 0.34825876355171204.\n",
      "Step 1000:Training set accuracy 0.9296875,cost 0.251924991607666.\n",
      "Step 1200:Training set accuracy 0.9375,cost 0.19416016340255737.\n",
      "Step 1400:Training set accuracy 0.96875,cost 0.1243765652179718.\n",
      "Step 1600:Training set accuracy 0.9375,cost 0.2011130452156067.\n",
      "Step 1800:Training set accuracy 0.984375,cost 0.08927571773529053.\n",
      "Step 2000:Training set accuracy 0.9609375,cost 0.16794393956661224.\n",
      "Step 2200:Training set accuracy 0.9140625,cost 0.23392076790332794.\n",
      "Step 2400:Training set accuracy 0.953125,cost 0.17082609236240387.\n",
      "Step 2600:Training set accuracy 0.953125,cost 0.15126438438892365.\n",
      "Step 2800:Training set accuracy 0.9453125,cost 0.23166316747665405.\n",
      "Step 3000:Training set accuracy 0.9453125,cost 0.15597593784332275.\n",
      "Step 3200:Training set accuracy 0.9453125,cost 0.11606389284133911.\n",
      "Step 3400:Training set accuracy 0.9765625,cost 0.06365194916725159.\n",
      "Step 3600:Training set accuracy 0.9453125,cost 0.18824055790901184.\n",
      "Step 3800:Training set accuracy 0.9765625,cost 0.08969835937023163.\n",
      "Step 4000:Training set accuracy 0.984375,cost 0.05095437169075012.\n",
      "Step 4200:Training set accuracy 0.9609375,cost 0.13385023176670074.\n",
      "Step 4400:Training set accuracy 0.9609375,cost 0.10179951786994934.\n",
      "Step 4600:Training set accuracy 0.9609375,cost 0.09208378940820694.\n",
      "Step 4800:Training set accuracy 0.9765625,cost 0.10557565838098526.\n",
      "Step 5000:Training set accuracy 0.984375,cost 0.06409947574138641.\n",
      "Step 20:Test set acuracy 1.0,cost 0.0363110676407814.\n",
      "Step 40:Test set acuracy 0.9599999785423279,cost 0.1466691941022873.\n",
      "Step 60:Test set acuracy 0.9599999785423279,cost 0.04125981405377388.\n",
      "Step 80:Test set acuracy 0.8999999761581421,cost 0.22707214951515198.\n",
      "Step 100:Test set acuracy 0.9599999785423279,cost 0.11548706144094467.\n",
      "Step 120:Test set acuracy 0.9599999785423279,cost 0.060300540179014206.\n",
      "Step 140:Test set acuracy 0.9399999976158142,cost 0.09510991722345352.\n",
      "Step 160:Test set acuracy 0.9800000190734863,cost 0.04704246670007706.\n",
      "Step 180:Test set acuracy 0.9800000190734863,cost 0.08096040785312653.\n",
      "Step 200:Test set acuracy 0.9599999785423279,cost 0.16657991707324982.\n",
      "Test accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "mnist_rnn_classfication(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n",
      "Training data shape:  (55000, 784)\n",
      "Test data shape:  (10000, 784)\n",
      "Validation data shape: (5000, 784)\n",
      "Training label shape:  (55000, 10)\n",
      " gru :\n",
      "hidden:  (?, 128)\n",
      "Step 200:Training set accuracy 0.6796875,cost 1.1145763397216797.\n",
      "Step 400:Training set accuracy 0.8515625,cost 0.4700564742088318.\n",
      "Step 600:Training set accuracy 0.8828125,cost 0.4241541028022766.\n",
      "Step 800:Training set accuracy 0.90625,cost 0.2860753536224365.\n",
      "Step 1000:Training set accuracy 0.96875,cost 0.15906667709350586.\n",
      "Step 1200:Training set accuracy 0.9375,cost 0.2300870716571808.\n",
      "Step 1400:Training set accuracy 0.96875,cost 0.13639217615127563.\n",
      "Step 1600:Training set accuracy 0.96875,cost 0.19322097301483154.\n",
      "Step 1800:Training set accuracy 0.953125,cost 0.11029686778783798.\n",
      "Step 2000:Training set accuracy 0.96875,cost 0.13793165981769562.\n",
      "Step 2200:Training set accuracy 0.9609375,cost 0.12568770349025726.\n",
      "Step 2400:Training set accuracy 0.9609375,cost 0.09468626976013184.\n",
      "Step 2600:Training set accuracy 0.984375,cost 0.0859285444021225.\n",
      "Step 2800:Training set accuracy 0.953125,cost 0.09827475994825363.\n",
      "Step 3000:Training set accuracy 0.96875,cost 0.0903557613492012.\n",
      "Step 3200:Training set accuracy 0.9609375,cost 0.10038982331752777.\n",
      "Step 3400:Training set accuracy 0.9765625,cost 0.06525495648384094.\n",
      "Step 3600:Training set accuracy 1.0,cost 0.04407460615038872.\n",
      "Step 3800:Training set accuracy 0.984375,cost 0.04462694004178047.\n",
      "Step 4000:Training set accuracy 0.96875,cost 0.11688562482595444.\n",
      "Step 4200:Training set accuracy 0.9609375,cost 0.10889427363872528.\n",
      "Step 4400:Training set accuracy 0.984375,cost 0.07985270023345947.\n",
      "Step 4600:Training set accuracy 0.96875,cost 0.10465443134307861.\n",
      "Step 4800:Training set accuracy 0.984375,cost 0.04144136235117912.\n",
      "Step 5000:Training set accuracy 0.9921875,cost 0.08044322580099106.\n",
      "Step 20:Test set acuracy 1.0,cost 0.01149576622992754.\n",
      "Step 40:Test set acuracy 1.0,cost 0.03751320019364357.\n",
      "Step 60:Test set acuracy 0.9200000166893005,cost 0.20903119444847107.\n",
      "Step 80:Test set acuracy 0.9800000190734863,cost 0.056912846863269806.\n",
      "Step 100:Test set acuracy 1.0,cost 0.005873525515198708.\n",
      "Step 120:Test set acuracy 0.9599999785423279,cost 0.11415181308984756.\n",
      "Step 140:Test set acuracy 0.9800000190734863,cost 0.09186604619026184.\n",
      "Step 160:Test set acuracy 1.0,cost 0.01656815968453884.\n",
      "Step 180:Test set acuracy 0.9800000190734863,cost 0.05469827353954315.\n",
      "Step 200:Test set acuracy 0.9599999785423279,cost 0.08960428088903427.\n",
      "Test accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "mnist_rnn_classfication(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n",
      "Training data shape:  (55000, 784)\n",
      "Test data shape:  (10000, 784)\n",
      "Validation data shape: (5000, 784)\n",
      "Training label shape:  (55000, 10)\n",
      " LSTM  gru :\n",
      "hidden:  (?, 256)\n",
      "Step 200:Training set accuracy 0.7265625,cost 0.8784635066986084.\n",
      "Step 400:Training set accuracy 0.8359375,cost 0.5293391942977905.\n",
      "Step 600:Training set accuracy 0.9140625,cost 0.34353768825531006.\n",
      "Step 800:Training set accuracy 0.921875,cost 0.2659567892551422.\n",
      "Step 1000:Training set accuracy 0.9453125,cost 0.21912038326263428.\n",
      "Step 1200:Training set accuracy 0.953125,cost 0.13806204497814178.\n",
      "Step 1400:Training set accuracy 0.96875,cost 0.14214468002319336.\n",
      "Step 1600:Training set accuracy 0.90625,cost 0.23859617114067078.\n",
      "Step 1800:Training set accuracy 0.96875,cost 0.11952529847621918.\n",
      "Step 2000:Training set accuracy 0.96875,cost 0.15004537999629974.\n",
      "Step 2200:Training set accuracy 0.9375,cost 0.24987833201885223.\n",
      "Step 2400:Training set accuracy 0.9375,cost 0.21855446696281433.\n",
      "Step 2600:Training set accuracy 0.9765625,cost 0.09080785512924194.\n",
      "Step 2800:Training set accuracy 0.984375,cost 0.08515241742134094.\n",
      "Step 3000:Training set accuracy 0.9765625,cost 0.0639151781797409.\n",
      "Step 3200:Training set accuracy 0.9453125,cost 0.21276000142097473.\n",
      "Step 3400:Training set accuracy 0.9765625,cost 0.055135246366262436.\n",
      "Step 3600:Training set accuracy 0.984375,cost 0.054298900067806244.\n",
      "Step 3800:Training set accuracy 0.9921875,cost 0.05694584548473358.\n",
      "Step 4000:Training set accuracy 0.9609375,cost 0.0842582955956459.\n",
      "Step 4200:Training set accuracy 0.9765625,cost 0.07192200422286987.\n",
      "Step 4400:Training set accuracy 0.9921875,cost 0.05204625800251961.\n",
      "Step 4600:Training set accuracy 0.984375,cost 0.04451192170381546.\n",
      "Step 4800:Training set accuracy 0.9765625,cost 0.11023275554180145.\n",
      "Step 5000:Training set accuracy 0.984375,cost 0.02802453748881817.\n",
      "Step 20:Test set acuracy 0.8799999952316284,cost 0.29553931951522827.\n",
      "Step 40:Test set acuracy 1.0,cost 0.03733988478779793.\n",
      "Step 60:Test set acuracy 0.9800000190734863,cost 0.13636614382266998.\n",
      "Step 80:Test set acuracy 0.9599999785423279,cost 0.09851878136396408.\n",
      "Step 100:Test set acuracy 0.9599999785423279,cost 0.11542043089866638.\n",
      "Step 120:Test set acuracy 1.0,cost 0.025232849642634392.\n",
      "Step 140:Test set acuracy 0.9599999785423279,cost 0.07011009007692337.\n",
      "Step 160:Test set acuracy 0.9399999976158142,cost 0.13021473586559296.\n",
      "Step 180:Test set acuracy 1.0,cost 0.02142927423119545.\n",
      "Step 200:Test set acuracy 1.0,cost 0.008913228288292885.\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "mnist_rnn_classfication(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mnist_rnn_classfication(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n",
      "Training data shape:  (55000, 784)\n",
      "Test data shape:  (10000, 784)\n",
      "Validation data shape: (5000, 784)\n",
      "Training label shape:  (55000, 10)\n",
      " LSTM :\n",
      "hidden:  (?, 128)\n",
      "Step 200:Training set accuracy 0.6953125,cost 0.9691199064254761.\n",
      "Step 400:Training set accuracy 0.8125,cost 0.5500832796096802.\n",
      "Step 600:Training set accuracy 0.84375,cost 0.49039405584335327.\n",
      "Step 800:Training set accuracy 0.9140625,cost 0.30988046526908875.\n",
      "Step 1000:Training set accuracy 0.9453125,cost 0.20493175089359283.\n",
      "Step 1200:Training set accuracy 0.921875,cost 0.19838997721672058.\n",
      "Step 1400:Training set accuracy 0.9453125,cost 0.1838347613811493.\n",
      "Step 1600:Training set accuracy 0.921875,cost 0.26599806547164917.\n",
      "Step 1800:Training set accuracy 0.921875,cost 0.2149784117937088.\n",
      "Step 2000:Training set accuracy 0.9765625,cost 0.0886395201086998.\n",
      "Step 2200:Training set accuracy 0.953125,cost 0.1650148332118988.\n",
      "Step 2400:Training set accuracy 0.96875,cost 0.08817422389984131.\n",
      "Step 2600:Training set accuracy 0.9453125,cost 0.13566163182258606.\n",
      "Step 2800:Training set accuracy 0.921875,cost 0.25040459632873535.\n",
      "Step 3000:Training set accuracy 0.9609375,cost 0.09785620868206024.\n",
      "Step 3200:Training set accuracy 0.9375,cost 0.14596903324127197.\n",
      "Step 3400:Training set accuracy 0.9609375,cost 0.10882925987243652.\n",
      "Step 3600:Training set accuracy 0.96875,cost 0.11573290079832077.\n",
      "Step 3800:Training set accuracy 0.9765625,cost 0.08909265697002411.\n",
      "Step 4000:Training set accuracy 0.9609375,cost 0.1314947009086609.\n",
      "Step 4200:Training set accuracy 0.9609375,cost 0.0946534126996994.\n",
      "Step 4400:Training set accuracy 0.96875,cost 0.06766320019960403.\n",
      "Step 4600:Training set accuracy 0.984375,cost 0.07021500170230865.\n",
      "Step 4800:Training set accuracy 0.984375,cost 0.048142265528440475.\n",
      "Step 5000:Training set accuracy 0.96875,cost 0.11095014959573746.\n",
      "Step 20:Test set acuracy 0.9800000190734863,cost 0.0638972744345665.\n",
      "Step 40:Test set acuracy 0.9800000190734863,cost 0.08732063323259354.\n",
      "Step 60:Test set acuracy 0.9800000190734863,cost 0.08360324054956436.\n",
      "Step 80:Test set acuracy 0.9800000190734863,cost 0.13078606128692627.\n",
      "Step 100:Test set acuracy 0.9800000190734863,cost 0.0696655809879303.\n",
      "Step 120:Test set acuracy 0.9399999976158142,cost 0.10806883871555328.\n",
      "Step 140:Test set acuracy 0.9800000190734863,cost 0.04322562366724014.\n",
      "Step 160:Test set acuracy 1.0,cost 0.03024516999721527.\n",
      "Step 180:Test set acuracy 0.9800000190734863,cost 0.04705241322517395.\n",
      "Step 200:Test set acuracy 1.0,cost 0.01537986472249031.\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "mnist_rnn_classfication(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n",
      "Training data shape:  (55000, 784)\n",
      "Test data shape:  (10000, 784)\n",
      "Validation data shape: (5000, 784)\n",
      "Training label shape:  (55000, 10)\n",
      " gru :\n",
      "hidden:  (?, 128)\n",
      "Step 200:Training set accuracy 0.703125,cost 0.9517717361450195.\n",
      "Step 400:Training set accuracy 0.8359375,cost 0.5352016687393188.\n",
      "Step 600:Training set accuracy 0.8984375,cost 0.3318464756011963.\n",
      "Step 800:Training set accuracy 0.9375,cost 0.20308233797550201.\n",
      "Step 1000:Training set accuracy 0.9296875,cost 0.21779941022396088.\n",
      "Step 1200:Training set accuracy 0.9609375,cost 0.1380101442337036.\n",
      "Step 1400:Training set accuracy 0.9453125,cost 0.16950106620788574.\n",
      "Step 1600:Training set accuracy 0.953125,cost 0.22029022872447968.\n",
      "Step 1800:Training set accuracy 0.9375,cost 0.19507485628128052.\n",
      "Step 2000:Training set accuracy 0.984375,cost 0.0671498030424118.\n",
      "Step 2200:Training set accuracy 0.9609375,cost 0.16339470446109772.\n",
      "Step 2400:Training set accuracy 0.9609375,cost 0.0844959020614624.\n",
      "Step 2600:Training set accuracy 0.984375,cost 0.0782763734459877.\n",
      "Step 2800:Training set accuracy 0.9609375,cost 0.14837510883808136.\n",
      "Step 3000:Training set accuracy 0.9765625,cost 0.05481628328561783.\n",
      "Step 3200:Training set accuracy 0.9375,cost 0.22715899348258972.\n",
      "Step 3400:Training set accuracy 0.9609375,cost 0.10216228663921356.\n",
      "Step 3600:Training set accuracy 0.984375,cost 0.07192154973745346.\n",
      "Step 3800:Training set accuracy 0.9765625,cost 0.1033104658126831.\n",
      "Step 4000:Training set accuracy 0.9921875,cost 0.025246139615774155.\n",
      "Step 4200:Training set accuracy 0.984375,cost 0.054157719016075134.\n",
      "Step 4400:Training set accuracy 0.96875,cost 0.07362044602632523.\n",
      "Step 4600:Training set accuracy 0.9765625,cost 0.07853497564792633.\n",
      "Step 4800:Training set accuracy 0.984375,cost 0.04397640749812126.\n",
      "Step 5000:Training set accuracy 0.9921875,cost 0.05090408772230148.\n",
      "Step 20:Test set acuracy 0.9599999785423279,cost 0.2128104418516159.\n",
      "Step 40:Test set acuracy 0.9800000190734863,cost 0.0596647746860981.\n",
      "Step 60:Test set acuracy 0.9800000190734863,cost 0.07461276650428772.\n",
      "Step 80:Test set acuracy 1.0,cost 0.00952321570366621.\n",
      "Step 100:Test set acuracy 0.9800000190734863,cost 0.05312991514801979.\n",
      "Step 120:Test set acuracy 0.9800000190734863,cost 0.1055983453989029.\n",
      "Step 140:Test set acuracy 0.9800000190734863,cost 0.05793612077832222.\n",
      "Step 160:Test set acuracy 0.9800000190734863,cost 0.04366174712777138.\n",
      "Step 180:Test set acuracy 0.9599999785423279,cost 0.07063750177621841.\n",
      "Step 200:Test set acuracy 1.0,cost 0.03214923292398453.\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "mnist_rnn_classfication(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n",
      "Training data shape:  (55000, 784)\n",
      "Test data shape:  (10000, 784)\n",
      "Validation data shape: (5000, 784)\n",
      "Training label shape:  (55000, 10)\n",
      "LSTMgru:\n",
      "hidden:  (?, 256)\n",
      "Step 200:Training set accuracy 0.671875,cost 0.9617934226989746.\n",
      "Step 400:Training set accuracy 0.8203125,cost 0.6066590547561646.\n",
      "Step 600:Training set accuracy 0.8359375,cost 0.4771036207675934.\n",
      "Step 800:Training set accuracy 0.9140625,cost 0.3374989628791809.\n",
      "Step 1000:Training set accuracy 0.9296875,cost 0.23284170031547546.\n",
      "Step 1200:Training set accuracy 0.9453125,cost 0.21570925414562225.\n",
      "Step 1400:Training set accuracy 0.890625,cost 0.3986506462097168.\n",
      "Step 1600:Training set accuracy 0.921875,cost 0.21539457142353058.\n",
      "Step 1800:Training set accuracy 0.9609375,cost 0.1220054030418396.\n",
      "Step 2000:Training set accuracy 0.9765625,cost 0.08234090358018875.\n",
      "Step 2200:Training set accuracy 0.96875,cost 0.11214075982570648.\n",
      "Step 2400:Training set accuracy 0.9765625,cost 0.06748177111148834.\n",
      "Step 2600:Training set accuracy 0.953125,cost 0.13694339990615845.\n",
      "Step 2800:Training set accuracy 0.9609375,cost 0.15748433768749237.\n",
      "Step 3000:Training set accuracy 0.9609375,cost 0.11870486289262772.\n",
      "Step 3200:Training set accuracy 0.9453125,cost 0.13515779376029968.\n",
      "Step 3400:Training set accuracy 0.984375,cost 0.06323497742414474.\n",
      "Step 3600:Training set accuracy 0.96875,cost 0.1073695570230484.\n",
      "Step 3800:Training set accuracy 0.9765625,cost 0.1003006175160408.\n",
      "Step 4000:Training set accuracy 0.96875,cost 0.10448221117258072.\n",
      "Step 4200:Training set accuracy 0.96875,cost 0.10176635533571243.\n",
      "Step 4400:Training set accuracy 0.9609375,cost 0.12904055416584015.\n",
      "Step 4600:Training set accuracy 0.984375,cost 0.04621317982673645.\n",
      "Step 4800:Training set accuracy 0.96875,cost 0.06010064482688904.\n",
      "Step 5000:Training set accuracy 0.96875,cost 0.057838186621665955.\n",
      "Step 20:Test set acuracy 0.9599999785423279,cost 0.1513010561466217.\n",
      "Step 40:Test set acuracy 0.9399999976158142,cost 0.21225734055042267.\n",
      "Step 60:Test set acuracy 1.0,cost 0.011778806336224079.\n",
      "Step 80:Test set acuracy 1.0,cost 0.041491638869047165.\n",
      "Step 100:Test set acuracy 0.9399999976158142,cost 0.2380397617816925.\n",
      "Step 120:Test set acuracy 0.9599999785423279,cost 0.12118440866470337.\n",
      "Step 140:Test set acuracy 1.0,cost 0.041912827640771866.\n",
      "Step 160:Test set acuracy 0.8999999761581421,cost 0.3953807055950165.\n",
      "Step 180:Test set acuracy 1.0,cost 0.008698711171746254.\n",
      "Step 200:Test set acuracy 0.9399999976158142,cost 0.1666567027568817.\n",
      "Test accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "mnist_rnn_classfication(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n",
      "Training data shape:  (55000, 784)\n",
      "Test data shape:  (10000, 784)\n",
      "Validation data shape: (5000, 784)\n",
      "Training label shape:  (55000, 10)\n",
      " LSTM :\n",
      " hiddens: In <class 'list'> 28 (?, 256) (?, 256)\n",
      "hidden:  (?, 256)\n",
      "Step 200:Training set accuracy 0.484375,cost 1.5694324970245361.\n",
      "Step 400:Training set accuracy 0.7265625,cost 0.8504409193992615.\n",
      "Step 600:Training set accuracy 0.8359375,cost 0.5505830645561218.\n",
      "Step 800:Training set accuracy 0.859375,cost 0.4742947518825531.\n",
      "Step 1000:Training set accuracy 0.8984375,cost 0.36897891759872437.\n",
      "Step 1200:Training set accuracy 0.875,cost 0.31410908699035645.\n",
      "Step 1400:Training set accuracy 0.9375,cost 0.25519275665283203.\n",
      "Step 1600:Training set accuracy 0.890625,cost 0.3879632353782654.\n",
      "Step 1800:Training set accuracy 0.921875,cost 0.2291780710220337.\n",
      "Step 2000:Training set accuracy 0.8984375,cost 0.2750020921230316.\n",
      "Step 2200:Training set accuracy 0.9375,cost 0.21196727454662323.\n",
      "Step 2400:Training set accuracy 0.9609375,cost 0.141217440366745.\n",
      "Step 2600:Training set accuracy 0.953125,cost 0.1911795437335968.\n",
      "Step 2800:Training set accuracy 0.9375,cost 0.18099337816238403.\n",
      "Step 3000:Training set accuracy 0.9609375,cost 0.10889900475740433.\n",
      "Step 3200:Training set accuracy 0.96875,cost 0.13356611132621765.\n",
      "Step 3400:Training set accuracy 0.9296875,cost 0.25221410393714905.\n",
      "Step 3600:Training set accuracy 0.9765625,cost 0.1303906887769699.\n",
      "Step 3800:Training set accuracy 0.9453125,cost 0.22050410509109497.\n",
      "Step 4000:Training set accuracy 0.9765625,cost 0.10673712193965912.\n",
      "Step 4200:Training set accuracy 0.953125,cost 0.1587720811367035.\n",
      "Step 4400:Training set accuracy 0.9765625,cost 0.12264986336231232.\n",
      "Step 4600:Training set accuracy 0.9765625,cost 0.08544041216373444.\n",
      "Step 4800:Training set accuracy 0.96875,cost 0.0807027593255043.\n",
      "Step 5000:Training set accuracy 0.953125,cost 0.12721741199493408.\n",
      "Step 20:Test set acuracy 1.0,cost 0.036196619272232056.\n",
      "Step 40:Test set acuracy 1.0,cost 0.019056186079978943.\n",
      "Step 60:Test set acuracy 0.9599999785423279,cost 0.09301414340734482.\n",
      "Step 80:Test set acuracy 0.9800000190734863,cost 0.11683644354343414.\n",
      "Step 100:Test set acuracy 0.9599999785423279,cost 0.12623044848442078.\n",
      "Step 120:Test set acuracy 0.9599999785423279,cost 0.04920990392565727.\n",
      "Step 140:Test set acuracy 0.9599999785423279,cost 0.18397507071495056.\n",
      "Step 160:Test set acuracy 0.9599999785423279,cost 0.11192505061626434.\n",
      "Step 180:Test set acuracy 0.9399999976158142,cost 0.0969986617565155.\n",
      "Step 200:Test set acuracy 0.9200000166893005,cost 0.20207440853118896.\n",
      "Test accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "mnist_rnn_classfication(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n",
      "Training data shape:  (55000, 784)\n",
      "Test data shape:  (10000, 784)\n",
      "Validation data shape: (5000, 784)\n",
      "Training label shape:  (55000, 10)\n",
      " LSTM :\n",
      " hiddens:\\ n <class 'tuple'> 2 (?, 28, 128) (?, 28, 128)\n",
      "hidden:  (?, 256)\n",
      "Step 200:Training set accuracy 0.5703125,cost 1.3276389837265015.\n",
      "Step 400:Training set accuracy 0.7421875,cost 0.8264073133468628.\n",
      "Step 600:Training set accuracy 0.859375,cost 0.5038665533065796.\n",
      "Step 800:Training set accuracy 0.84375,cost 0.5350511074066162.\n",
      "Step 1000:Training set accuracy 0.921875,cost 0.36744970083236694.\n",
      "Step 1200:Training set accuracy 0.921875,cost 0.34033241868019104.\n",
      "Step 1400:Training set accuracy 0.9296875,cost 0.3441024422645569.\n",
      "Step 1600:Training set accuracy 0.890625,cost 0.31042593717575073.\n",
      "Step 1800:Training set accuracy 0.9453125,cost 0.19500088691711426.\n",
      "Step 2000:Training set accuracy 0.9609375,cost 0.14317908883094788.\n",
      "Step 2200:Training set accuracy 0.9453125,cost 0.18492648005485535.\n",
      "Step 2400:Training set accuracy 0.953125,cost 0.18408790230751038.\n",
      "Step 2600:Training set accuracy 0.9765625,cost 0.11166131496429443.\n",
      "Step 2800:Training set accuracy 0.96875,cost 0.15301713347434998.\n",
      "Step 3000:Training set accuracy 0.9609375,cost 0.17705942690372467.\n",
      "Step 3200:Training set accuracy 0.96875,cost 0.13181425631046295.\n",
      "Step 3400:Training set accuracy 0.96875,cost 0.0679640844464302.\n",
      "Step 3600:Training set accuracy 0.9453125,cost 0.14600065350532532.\n",
      "Step 3800:Training set accuracy 0.9609375,cost 0.1445489227771759.\n",
      "Step 4000:Training set accuracy 0.953125,cost 0.12815599143505096.\n",
      "Step 4200:Training set accuracy 0.96875,cost 0.11872656643390656.\n",
      "Step 4400:Training set accuracy 0.9609375,cost 0.10868658870458603.\n",
      "Step 4600:Training set accuracy 0.9375,cost 0.15545940399169922.\n",
      "Step 4800:Training set accuracy 0.953125,cost 0.14594261348247528.\n",
      "Step 5000:Training set accuracy 0.9921875,cost 0.058103304356336594.\n",
      "Step 20:Test set acuracy 0.9599999785423279,cost 0.21682269871234894.\n",
      "Step 40:Test set acuracy 0.9800000190734863,cost 0.06520439684391022.\n",
      "Step 60:Test set acuracy 0.9599999785423279,cost 0.14474622905254364.\n",
      "Step 80:Test set acuracy 0.9399999976158142,cost 0.1403670459985733.\n",
      "Step 100:Test set acuracy 1.0,cost 0.01731519214808941.\n",
      "Step 120:Test set acuracy 0.9800000190734863,cost 0.08797099441289902.\n",
      "Step 140:Test set acuracy 0.9399999976158142,cost 0.17491333186626434.\n",
      "Step 160:Test set acuracy 0.9599999785423279,cost 0.06924504786729813.\n",
      "Step 180:Test set acuracy 0.9599999785423279,cost 0.09285348653793335.\n",
      "Step 200:Test set acuracy 0.9599999785423279,cost 0.17220935225486755.\n",
      "Test accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "mnist_rnn_classfication(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n",
      "Training data shape:  (55000, 784)\n",
      "Test data shape:  (10000, 784)\n",
      "Validation data shape: (5000, 784)\n",
      "Training label shape:  (55000, 10)\n",
      " LSTM :\n",
      "hiddens:\n",
      " <class 'list'> 28 (?, 256) (?, 256)\n",
      "hidden:  (?, 256)\n",
      "Step 200:Training set accuracy 0.7421875,cost 0.7567585706710815.\n",
      "Step 400:Training set accuracy 0.859375,cost 0.41720277070999146.\n",
      "Step 600:Training set accuracy 0.921875,cost 0.26791882514953613.\n",
      "Step 800:Training set accuracy 0.8984375,cost 0.36067262291908264.\n",
      "Step 1000:Training set accuracy 0.9375,cost 0.19441965222358704.\n",
      "Step 1200:Training set accuracy 0.890625,cost 0.325069397687912.\n",
      "Step 1400:Training set accuracy 0.984375,cost 0.06889857351779938.\n",
      "Step 1600:Training set accuracy 0.9375,cost 0.2288302779197693.\n",
      "Step 1800:Training set accuracy 0.9375,cost 0.1652437448501587.\n",
      "Step 2000:Training set accuracy 0.96875,cost 0.11208699643611908.\n",
      "Step 2200:Training set accuracy 0.96875,cost 0.1357964724302292.\n",
      "Step 2400:Training set accuracy 0.9765625,cost 0.11816830188035965.\n",
      "Step 2600:Training set accuracy 0.9765625,cost 0.07093051075935364.\n",
      "Step 2800:Training set accuracy 0.953125,cost 0.14946874976158142.\n",
      "Step 3000:Training set accuracy 0.9765625,cost 0.05049319565296173.\n",
      "Step 3200:Training set accuracy 0.96875,cost 0.0878763347864151.\n",
      "Step 3400:Training set accuracy 0.984375,cost 0.054460711777210236.\n",
      "Step 3600:Training set accuracy 0.9765625,cost 0.06751784682273865.\n",
      "Step 3800:Training set accuracy 0.96875,cost 0.07400781661272049.\n",
      "Step 4000:Training set accuracy 0.96875,cost 0.09402674436569214.\n",
      "Step 4200:Training set accuracy 0.984375,cost 0.043863993138074875.\n",
      "Step 4400:Training set accuracy 0.9765625,cost 0.06240958347916603.\n",
      "Step 4600:Training set accuracy 0.984375,cost 0.03480493277311325.\n",
      "Step 4800:Training set accuracy 0.96875,cost 0.05509372800588608.\n",
      "Step 5000:Training set accuracy 1.0,cost 0.022838719189167023.\n",
      "Step 20:Test set acuracy 0.9800000190734863,cost 0.10784315317869186.\n",
      "Step 40:Test set acuracy 0.9800000190734863,cost 0.050896964967250824.\n",
      "Step 60:Test set acuracy 1.0,cost 0.0173866655677557.\n",
      "Step 80:Test set acuracy 0.9200000166893005,cost 0.2824746370315552.\n",
      "Step 100:Test set acuracy 0.9599999785423279,cost 0.2443682700395584.\n",
      "Step 120:Test set acuracy 0.9800000190734863,cost 0.06470901519060135.\n",
      "Step 140:Test set acuracy 0.8999999761581421,cost 0.10579631477594376.\n",
      "Step 160:Test set acuracy 0.9599999785423279,cost 0.13634586334228516.\n",
      "Step 180:Test set acuracy 0.9599999785423279,cost 0.07278737425804138.\n",
      "Step 200:Test set acuracy 0.9800000190734863,cost 0.05065375193953514.\n",
      "Test accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "mnist_rnn_classfication(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n",
      "Training data shape:  (55000, 784)\n",
      "Test data shape:  (10000, 784)\n",
      "Validation data shape: (5000, 784)\n",
      "Training label shape:  (55000, 10)\n",
      " LSTM :\n",
      " hiddens: \n",
      " <class 'tensorflow.python.framework.ops.Tensor'> (?, 28, 256)\n",
      "hidden:  (?, 256)\n",
      "Step 200:Training set accuracy 0.765625,cost 0.8243806958198547.\n",
      "Step 400:Training set accuracy 0.8671875,cost 0.42670461535453796.\n",
      "Step 600:Training set accuracy 0.921875,cost 0.2567307651042938.\n",
      "Step 800:Training set accuracy 0.921875,cost 0.23258858919143677.\n",
      "Step 1000:Training set accuracy 0.9140625,cost 0.26583343744277954.\n",
      "Step 1200:Training set accuracy 0.9375,cost 0.22862544655799866.\n",
      "Step 1400:Training set accuracy 0.953125,cost 0.13459211587905884.\n",
      "Step 1600:Training set accuracy 0.953125,cost 0.11234302818775177.\n",
      "Step 1800:Training set accuracy 0.9609375,cost 0.11535335332155228.\n",
      "Step 2000:Training set accuracy 0.9453125,cost 0.1368052363395691.\n",
      "Step 2200:Training set accuracy 0.9921875,cost 0.023451611399650574.\n",
      "Step 2400:Training set accuracy 0.96875,cost 0.12380938977003098.\n",
      "Step 2600:Training set accuracy 0.9765625,cost 0.10977470874786377.\n",
      "Step 2800:Training set accuracy 0.9921875,cost 0.05130395293235779.\n",
      "Step 3000:Training set accuracy 0.953125,cost 0.17080476880073547.\n",
      "Step 3200:Training set accuracy 0.9765625,cost 0.10365257412195206.\n",
      "Step 3400:Training set accuracy 0.9609375,cost 0.10453912615776062.\n",
      "Step 3600:Training set accuracy 0.984375,cost 0.08247269690036774.\n",
      "Step 3800:Training set accuracy 0.9765625,cost 0.05371901020407677.\n",
      "Step 4000:Training set accuracy 0.9921875,cost 0.027316663414239883.\n",
      "Step 4200:Training set accuracy 0.9765625,cost 0.0728999525308609.\n",
      "Step 4400:Training set accuracy 0.9765625,cost 0.06636644899845123.\n",
      "Step 4600:Training set accuracy 0.9921875,cost 0.027528900653123856.\n",
      "Step 4800:Training set accuracy 0.9921875,cost 0.04089196026325226.\n",
      "Step 5000:Training set accuracy 0.984375,cost 0.03876197710633278.\n",
      "Step 20:Test set acuracy 1.0,cost 0.02078086882829666.\n",
      "Step 40:Test set acuracy 0.9599999785423279,cost 0.09755611419677734.\n",
      "Step 60:Test set acuracy 0.9800000190734863,cost 0.0642985925078392.\n",
      "Step 80:Test set acuracy 1.0,cost 0.010082019492983818.\n",
      "Step 100:Test set acuracy 1.0,cost 0.013981715776026249.\n",
      "Step 120:Test set acuracy 0.9599999785423279,cost 0.059130553156137466.\n",
      "Step 140:Test set acuracy 1.0,cost 0.03341178596019745.\n",
      "Step 160:Test set acuracy 0.9599999785423279,cost 0.10023503005504608.\n",
      "Step 180:Test set acuracy 0.9800000190734863,cost 0.02935146726667881.\n",
      "Step 200:Test set acuracy 0.9599999785423279,cost 0.19469596445560455.\n",
      "Test accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "mnist_rnn_classfication(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
